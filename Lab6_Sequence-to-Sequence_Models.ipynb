{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiWHpgu2yUrm"
      },
      "source": [
        "<a \n",
        "href=\"https://colab.research.google.com/github/wingated/cs474_labs_f2019/blob/master/DL_Lab6.ipynb\"\n",
        "  target=\"_parent\">\n",
        "  <img\n",
        "    src=\"https://colab.research.google.com/assets/colab-badge.svg\"\n",
        "    alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cksgAH12XRjV"
      },
      "source": [
        "# Lab 6: Sequence-to-sequence models\n",
        "\n",
        "### Description:\n",
        "For this lab, you will code up the [char-rnn model of Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). This is a recurrent neural network that is trained probabilistically on sequences of characters, and that can then be used to sample new sequences that are like the original.\n",
        "\n",
        "This lab will help you develop several new skills, as well as understand some best practices needed for building large models. In addition, we'll be able to create networks that generate neat text!\n",
        "\n",
        "### Deliverable:\n",
        "- Fill in the code for the RNN (using PyTorch's built-in GRU).\n",
        "- Fill in the training loop\n",
        "- Fill in the evaluation loop. In this loop, rather than using a validation set, you will sample text from the RNN.\n",
        "- Implement your own GRU cell.\n",
        "- Train your RNN on a new domain of text (Star Wars, political speeches, etc. - have fun!)\n",
        "\n",
        "### Grading Standards:\n",
        "- 20% Implementation the RNN\n",
        "- 20% Implementation training loop\n",
        "- 20% Implementation of evaluation loop\n",
        "- 20% Implementation of your own GRU cell\n",
        "- 20% Training of your RNN on a domain of your choice\n",
        "\n",
        "### Tips:\n",
        "- Read through all the helper functions, run them, and make sure you understand what they are doing\n",
        "- At each stage, ask yourself: What should the dimensions of this tensor be? Should its data type be float or int? (int is called `long` in PyTorch)\n",
        "- Don't apply a softmax inside the RNN if you are using an nn.CrossEntropyLoss (this module already applies a softmax to its input).\n",
        "\n",
        "### Example Output:\n",
        "An example of my final samples are shown below (more detail in the\n",
        "final section of this writeup), after 150 passes through the data.\n",
        "Please generate about 15 samples for each dataset.\n",
        "\n",
        "<code>\n",
        "And ifte thin forgision forward thene over up to a fear not your\n",
        "And freitions, which is great God. Behold these are the loss sub\n",
        "And ache with the Lord hath bloes, which was done to the holy Gr\n",
        "And appeicis arm vinimonahites strong in name, to doth piseling \n",
        "And miniquithers these words, he commanded order not; neither sa\n",
        "And min for many would happine even to the earth, to said unto m\n",
        "And mie first be traditions? Behold, you, because it was a sound\n",
        "And from tike ended the Lamanites had administered, and I say bi\n",
        "</code>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2i_QpSsWG4c"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 0: Readings, data loading, and high level training\n",
        "\n",
        "---\n",
        "\n",
        "There is a tutorial here that will help build out scaffolding code, and get an understanding of using sequences in pytorch.\n",
        "\n",
        "* Read the following\n",
        "\n",
        "> * [Pytorch sequence-to-sequence tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) (Take note that you will not be implementing the encoder part of this tutorial.)\n",
        "* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7bdZWxvJrsx",
        "outputId": "29e38b3d-e4aa-45f4-904a-22a66efb581d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-03 15:56:06--  https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz\n",
            "Resolving piazza.com (piazza.com)... 18.214.211.171, 52.7.218.200, 3.221.126.233, ...\n",
            "Connecting to piazza.com (piazza.com)|18.214.211.171|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-uploads.piazza.com/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz [following]\n",
            "--2022-02-03 15:56:06--  https://cdn-uploads.piazza.com/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz\n",
            "Resolving cdn-uploads.piazza.com (cdn-uploads.piazza.com)... 13.249.137.61, 13.249.137.97, 13.249.137.78, ...\n",
            "Connecting to cdn-uploads.piazza.com (cdn-uploads.piazza.com)|13.249.137.61|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1533290 (1.5M) [application/x-gzip]\n",
            "Saving to: ‘./text_files.tar.gz’\n",
            "\n",
            "./text_files.tar.gz 100%[===================>]   1.46M  5.53MB/s    in 0.3s    \n",
            "\n",
            "2022-02-03 15:56:07 (5.53 MB/s) - ‘./text_files.tar.gz’ saved [1533290/1533290]\n",
            "\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.3.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "file_len = 2579888\n"
          ]
        }
      ],
      "source": [
        "! wget -O ./text_files.tar.gz 'https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz' \n",
        "! tar -xzf text_files.tar.gz\n",
        "! pip install unidecode\n",
        "! pip install torch\n",
        "\n",
        "import unidecode\n",
        "import string\n",
        "import random\n",
        "import re\n",
        " \n",
        "import pdb\n",
        " \n",
        "all_characters = string.printable\n",
        "n_characters = len(all_characters)\n",
        "file = unidecode.unidecode(open('./text_files/lotr.txt').read())\n",
        "file_len = len(file)\n",
        "print('file_len =', file_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxBeKeNjJ0NQ",
        "outputId": "9c041abd-360a-41e1-c008-79bdd928649d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "and paid little attention to Gandalf telling \n",
            "him of the customs of Gondor, and how the Lord of the City had beacons built \n",
            "on the tops of outlying hills along both borders of the great range, and \n",
            "mai\n"
          ]
        }
      ],
      "source": [
        "# This cell outputs some random chunk of text from the corpus. The random chunk\n",
        "  # is decided by taking a random number between 0, and the length of the file \n",
        "  # minus the chunk length (this is something predetermined by us; in this case it is\n",
        "  # 200). This gives us a random start index between 0 and 2,579,888 - 200.\n",
        "  # Next, we have an end index. This is calculated by the start_index (random) and the\n",
        "  # chunk length plus 1. This will give us a range of 200 characters.\n",
        "# (This method does not always give complete sentences and thoughts.)\n",
        "\n",
        "chunk_len = 200\n",
        " \n",
        "def random_chunk():\n",
        "  start_index = random.randint(0, file_len - chunk_len)\n",
        "  end_index = start_index + chunk_len + 1\n",
        "  return file[start_index:end_index]\n",
        "  \n",
        "print(random_chunk())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "On0_WitWJ99e",
        "outputId": "e918dd1e-3f99-49b1-d6e2-e96b2021043a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([10, 11, 12, 39, 40, 41], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# This cell turns out characters into numbers in the form of a tensor. \n",
        "  # (My guess is this will be used for one-hot encoding later.)\n",
        "\n",
        "import torch\n",
        "# Turn string into list of longs\n",
        "def char_tensor(string):\n",
        "  tensor = torch.zeros(len(string)).long()\n",
        "  for c in range(len(string)):\n",
        "      tensor[c] = all_characters.index(string[c])\n",
        "  return tensor.cuda()\n",
        "\n",
        "print(char_tensor('abcDEF'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYJPTLcaYmfI"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 4: Creating your own GRU cell \n",
        "\n",
        "**(Come back to this later - its defined here so that the GRU will be defined before it is used)**\n",
        "\n",
        "---\n",
        "\n",
        "The cell that you used in Part 1 was a pre-defined Pytorch layer. Now, write your own GRU class using the same parameters as the built-in Pytorch class does.\n",
        "\n",
        "Please try not to look at the GRU cell definition. The answer is right there in the code, and in theory, you could just cut-and-paste it. This bit is on your honor!\n",
        "\n",
        "**TODO:**\n",
        "* Create a custom GRU cell\n",
        "\n",
        "**DONE:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aavAv50ZKQ-F"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class GRU(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers):\n",
        "    super(GRU, self).__init__()\n",
        "\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    # For W_*r\n",
        "      # Where * represents either i or h. i stands for input, h stands for hidden.\n",
        "    self.W_ir = nn.ModuleList([nn.Linear(input_size, hidden_size) for _ in range(num_layers)])\n",
        "    self.W_hr = nn.ModuleList([nn.Linear(input_size, hidden_size) for _ in range(num_layers)])\n",
        "\n",
        "    # For W_*z\n",
        "    self.W_iz = nn.ModuleList([nn.Linear(input_size, hidden_size) for _ in range(num_layers)])\n",
        "    self.W_hz = nn.ModuleList([nn.Linear(input_size, hidden_size) for _ in range(num_layers)])\n",
        "\n",
        "    # For W_*n\n",
        "    self.W_in = nn.ModuleList([nn.Linear(input_size, hidden_size) for _ in range(num_layers)])\n",
        "    self.W_hn = nn.ModuleList([nn.Linear(input_size, hidden_size) for _ in range(num_layers)])\n",
        "\n",
        "    # Activation Functions\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.tanh = nn.Tanh()\n",
        "    \n",
        "  \n",
        "  def forward(self, inputs, hidden):\n",
        "    hidden_tensor = tensor.zeros(hidden.shape).cuda()\n",
        "    for i in range(self.num_layers):\n",
        "      hidden_input = hidden[i]\n",
        "      r_t = self.sigmoid(self.W_ir[i](inputs) + self.W_hr[i](hidden_input))\n",
        "      z_t = self.sigmoid(self.W_iz[i](inputs) + self.W_hz[i](hidden_input))\n",
        "      n_t = self.tanh(self.W_in[i](inputs) + self.W_hn[i](hidden_input))\n",
        "      h_t = torch.mul((1-z_t), n_t) + torch.mul(z_t, hidden_input)\n",
        "\n",
        "      del r_t, z_t, n_t\n",
        "\n",
        "      hidden_tensor[i] = h_t\n",
        "      inputs = h_t\n",
        "    return inputs, hidden_tensor\n",
        "   \n",
        "    # Each layer does the following:\n",
        "    # r_t = sigmoid(W_ir*x_t + b_ir + W_hr*h_(t-1) + b_hr)\n",
        "    # z_t = sigmoid(W_iz*x_t + b_iz + W_hz*h_(t-1) + b_hz)\n",
        "    # n_t = tanh(W_in*x_t + b_in + r_t**(W_hn*h_(t-1) + b_hn))\n",
        "    # h_(t) = (1 - z_t)**n_t + z_t**h_(t-1)\n",
        "    # Where ** is hadamard product (not matrix multiplication, but elementwise multiplication)\n",
        "    \n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtXdX-B_WiAY"
      },
      "source": [
        "---\n",
        "\n",
        "##  Part 1: Building a sequence to sequence model\n",
        "\n",
        "---\n",
        "\n",
        "Great! We have the data in a useable form. We can switch out which text file we are reading from, and trying to simulate.\n",
        "\n",
        "We now want to build out an RNN model, in this section, we will use all built in Pytorch pieces when building our RNN class.\n",
        "\n",
        "\n",
        "**TODO:**\n",
        "* Create an RNN class that extends from nn.Module.\n",
        "\n",
        "**DONE:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6tNdEnzWj5F"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "    super(RNN, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.n_layers = n_layers\n",
        "    \n",
        "    # more stuff here...\n",
        "    self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "    self.gru       = nn.GRU(hidden_size, hidden_size, n_layers)\n",
        "    self.out       = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, input_char, hidden):\n",
        "    # by reviewing the documentation, construct a forward function that properly uses the output\n",
        "    # of the GRU\n",
        "    output = self.embedding(input_char)\n",
        "    if len(output.size()) == 1:\n",
        "      output = output.unsqueeze(0).unsqueeze(0)\n",
        "    elif len(output.size()) == 2:\n",
        "      output = output.unsqueeze(0)\n",
        "    output = F.relu(output)\n",
        "    output, hidden = self.gru(output, hidden)\n",
        "    out_decoded = self.out(output).squeeze(0)  \n",
        "    return out_decoded, hidden\n",
        "\n",
        "  def init_hidden(self):\n",
        "    return torch.zeros(self.n_layers, 1, self.hidden_size).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrhXghEPKD-5"
      },
      "outputs": [],
      "source": [
        "def random_training_set():    \n",
        "  chunk = random_chunk()\n",
        "  inp = char_tensor(chunk[:-1])\n",
        "  target = char_tensor(chunk[1:])\n",
        "  return inp, target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpiGObbBX0Mr"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 2: Sample text and Training information\n",
        "\n",
        "---\n",
        "\n",
        "We now want to be able to train our network, and sample text after training.\n",
        "\n",
        "This function outlines how training a sequence style network goes. \n",
        "\n",
        "**TODO:**\n",
        "* Fill in the pieces.\n",
        "\n",
        "**DONE:**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ALC3Pf8Kbsi"
      },
      "outputs": [],
      "source": [
        "# NOTE: decoder_optimizer, decoder, and criterion will be defined below as global variables (see part 5).\n",
        "def train(inp, target):\n",
        "  ## initialize hidden layers, set up gradient and loss \n",
        "    # your code here\n",
        "  ## /\n",
        "  decoder_optimizer.zero_grad()\n",
        "  hidden = decoder.init_hidden()\n",
        "  loss = 0\n",
        "    \n",
        "  # more stuff here...\n",
        "  for i in range(len(inp)):\n",
        "    y_hat, hidden = decoder(inp[i], hidden)\n",
        "    loss += criterion(y_hat, target[i].unsqueeze(0))\n",
        "  loss.backward()\n",
        "  decoder_optimizer.step()\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN06NUu3YRlz"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 3: Sample text and Training information\n",
        "\n",
        "---\n",
        "\n",
        "You can at this time, if you choose, also write out your train loop boilerplate that samples random sequences and trains your RNN. This will be helpful to have working before writing your own GRU class.\n",
        "\n",
        "If you are finished training, or during training, and you want to sample from the network you may consider using the following function. If your RNN model is instantiated as `decoder`then this will probabilistically sample a sequence of length `predict_len`\n",
        "\n",
        "**TODO:**\n",
        "* Fill out the evaluate function to generate text frome a primed string\n",
        "\n",
        "**DONE:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-bp-OZ1KjNh"
      },
      "outputs": [],
      "source": [
        "def sample_outputs(output, temperature):\n",
        "    \"\"\"Takes in a vector of unnormalized probability weights and samples a character from the distribution\"\"\"\n",
        "    return torch.multinomial(torch.exp(output / temperature), 1)\n",
        "\n",
        "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
        "  ## initialize hidden state, initialize other useful variables\n",
        "  hidden = decoder.init_hidden()\n",
        "\n",
        "  # This loop primes the output and the hidden layers with the char_tensor function\n",
        "    # defined above and the prime_str that is present in the function parameters.\n",
        "  for char in prime_str:\n",
        "    output, hidden = decoder(char_tensor(char), hidden)\n",
        "\n",
        "  output_list = []\n",
        "  # This loop is very similar to the training. \n",
        "  for i in range(predict_len):\n",
        "    # You take a 'predicted' character index\n",
        "    character_index = sample_outputs(output, temperature)\n",
        "    # Send that index into the decoder (which is just the RNN model), and gives us the actual input\n",
        "      # and the hidden. \n",
        "    output, hidden = decoder(character_index, hidden)\n",
        "    # Appends those values to a list that is returned with the primer.\n",
        "    output_list.append(all_characters[character_index])\n",
        "  return prime_str + ''.join(output_list)\n",
        "  \n",
        "  ## /\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Du4AGA8PcFEW"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 4: (Create a GRU cell, requirements above)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFS2bpHSZEU6"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Part 5: Run it and generate some text!\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**TODO:** \n",
        "* Create some cool output\n",
        "\n",
        "**DONE:**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Assuming everything has gone well, you should be able to run the main function in the scaffold code, using either your custom GRU cell or the built in layer, and see output something like this. I trained on the “lotr.txt” dataset, using chunk_length=200, hidden_size=100 for 2000 epochs. These are the results, along with the prime string:\n",
        "\n",
        "---\n",
        "\n",
        " G:\n",
        " \n",
        " Gandalf was decrond. \n",
        "'All have lord you. Forward the road at least walk this is stuff, and \n",
        "went to the long grey housel-winding and kindled side was a sleep pleasuring, I do long \n",
        "row hrough. In  \n",
        "\n",
        " lo:\n",
        " \n",
        " lost death it. \n",
        "'The last of the gatherings and take you,' said Aragorn, shining out of the Gate. \n",
        "'Yes, as you there were remembaused to seen their pass, when? What \n",
        "said here, such seven an the sear \n",
        "\n",
        " lo:\n",
        " \n",
        " low, and frod to keepn \n",
        "Came of their most. But here priced doubtless to an Sam up is \n",
        "masters; he left hor as they are looked. And he could now the long to stout in the right fro horseless of \n",
        "the like \n",
        "\n",
        " I:\n",
        " \n",
        " I had been the \n",
        "in his eyes with the perushed to lest, if then only the ring and the legended \n",
        "of the less of the long they which as the \n",
        "enders of Orcovered and smood, and the p \n",
        "\n",
        " I:\n",
        " \n",
        " I they were not the lord of the hoomes. \n",
        "Home already well from the Elves. And he sat strength, and we \n",
        "housed out of the good of the days to the mountains from his perith. \n",
        "\n",
        "'Yess! Where though as if  \n",
        "\n",
        " Th:\n",
        " \n",
        " There yarden \n",
        "you would guard the hoor might. Far and then may was \n",
        "croties, too began to see the drumbred many line \n",
        "and was then hoard walk and they heart, and the chair of the \n",
        "Ents of way, might was \n",
        "\n",
        " G:\n",
        " \n",
        " Gandalf \n",
        "been lat of less the round of the stump; both and seemed to the trees and perished they \n",
        "lay are speered the less; and the wind the steep and have to she \n",
        "precious. There was in the oonly went \n",
        "\n",
        " wh:\n",
        " \n",
        " which went out of the door. \n",
        "Hull the King and of the The days of his brodo \n",
        "stumbler of the windard was a thing there, then it been shining langing \n",
        "to him poor land. They hands; though they seemed ou \n",
        "\n",
        " ra:\n",
        " \n",
        " rather,' have all the least deather \n",
        "down of the truven beginning to the house of sunk. \n",
        "'Nark shorts of the Eyes of the Gate your great nothing as Eret. \n",
        "'I wander trust horn, and there were not, it  \n",
        "\n",
        " I:\n",
        " \n",
        " I can have no mind \n",
        "together! Where don't may had one may little blung \n",
        "terrible to tales. And turn and Gandalf shall be not to as only the Cattring \n",
        "not stopped great the out them forms. On they she lo \n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nXFeCmdKodw"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "n_epochs = 5000\n",
        "print_every = 200\n",
        "plot_every = 10\n",
        "hidden_size = 200\n",
        "n_layers = 3\n",
        "lr = 0.001\n",
        " \n",
        "decoder = RNN(n_characters, hidden_size, n_characters, n_layers).cuda()\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        " \n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKfozqw-6eqb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9f5f166-7c18-4818-aea7-fd3250d09204"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[442.72340154647827 (200 9%) 374.9806]\n",
            "Wh and \n",
            "where eary far a lave \n",
            "yet orly I hand owfal stone \n",
            "and yintard.' \n",
            "\n",
            "'You all hall some hay as  \n",
            "\n",
            "[513.3835945129395 (400 19%) 320.7556]\n",
            "When there arked torntwing the wander in as a drown: \n",
            "you the granger stieln at the great a histens. L \n",
            "\n",
            "[583.9559464454651 (600 29%) 385.5910]\n",
            "What gan that \n",
            "were could not you have Pornather hope use a long seemed was gour hust. When there arss \n",
            "\n",
            "[654.5491192340851 (800 39%) 281.9439]\n",
            "What have nor reen that I wark of did now. \n",
            "\n",
            "Merry men and PorVwerry to point. You, yest their \n",
            "compin \n",
            "\n",
            "[724.8530113697052 (1000 49%) 303.7890]\n",
            "Whough old was gather that lashings sleak of \n",
            "hip to strighthant is not on besiled in horse them. \n",
            "\n",
            "'I \n",
            "\n",
            "[796.0965142250061 (1200 59%) 301.0591]\n",
            "What has answered with his waten furthing here. \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "The falled have flrained the listed and of \n",
            "\n",
            "[867.1562223434448 (1400 69%) 329.8472]\n",
            "Wheed Back, and Gomlepfled busied, may, \n",
            "The inough got that not had spenking some me in the thought f \n",
            "\n",
            "[936.9517092704773 (1600 79%) 357.3766]\n",
            "While had be a should then. They have speed to think he young up, but stoop staff, but off they pondy  \n",
            "\n",
            "[1009.4944798946381 (1800 89%) 300.2589]\n",
            "White to the firely, I do not \n",
            "the follow the air book of the Mill of the sam;. 'They'rath us among th \n",
            "\n",
            "[1088.5653524398804 (2000 99%) 344.6728]\n",
            "White happen folk, when you are more, it was mores under? The \n",
            "mountains, and creaming Tower and all t \n",
            "\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 2001\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  loss_ = train(*random_training_set())       \n",
        "  loss_avg += loss_\n",
        "\n",
        "  if epoch % print_every == 0:\n",
        "      print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n",
        "      print(evaluate('Wh', 100), '\\n')\n",
        "\n",
        "  if epoch % plot_every == 0:\n",
        "      all_losses.append(loss_avg / plot_every)\n",
        "      loss_avg = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ee0so6aKJ5L8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4496c5e-ded5-4c8a-9f2e-3e365872a8a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ra\n",
            " ran him, or the stood \n",
            "you folk the mealanns to go it long with bet outsing. The innothers many some all visking pass to every' he will not eventret, and the amward the commanded to the clear to the Ber \n",
            "\n",
            " he\n",
            " he disp. But your present, answered we must good was head, the wood, \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Sam or grown, it was \n",
            "road, you our chain, and also the poate and command. The windoo was of the wind he winved powent me. \n",
            "\n",
            "And \n",
            "\n",
            " ca\n",
            " came be told my nist my blike other \n",
            "many anyway, if a round to the lings the looked; \n",
            "and came like the enemy upon precious. Beyonded looked round or among the strength driven at \n",
            "linted once hide, or  \n",
            "\n",
            " G\n",
            " Gamped hew see much: loud down \n",
            "if us men under, and. On persuncies unthaned; and \n",
            "time thing be, noundo\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sky a sorn Staith. But my some manders age, as if some is folk them, yach on the mountains \n",
            "\n",
            " G\n",
            " Gandalf. For me that is he will fear Great listended words \n",
            "like the Enemy to deed plapped, \n",
            "to the mountains their Birtly the Butter near as if the \n",
            "Came ditter \n",
            "strength away the Northern that was \n",
            "t \n",
            "\n",
            " he\n",
            " heirided, and some or seemed on it.' \n",
            "\n",
            "Then he had \n",
            "quite climbing into the grey this plain wise, and the looked a moment in a strfaning. The could the from beard returned back had came lightly; sunsent \n",
            "\n",
            " he\n",
            " he coming and croved up. \n",
            "\n",
            "The wind in men to the were heffing with coment, and all be the mounting \n",
            "sumproaning from the \n",
            "halling upon him, sumpusas of much ago to the broken beart which points under t \n",
            "\n",
            " ca\n",
            " came about the wall that a trought through the Commold somers \n",
            "storight. \n",
            "\n",
            "And he \n",
            "came for among mardened as if his command. The commandil the \n",
            "bourder bowed plain: his still in \n",
            "now, less. \n",
            "\n",
            "The tuck  \n",
            "\n",
            " ra\n",
            " rain, to more. \n",
            "\n",
            "There was shielded was \n",
            "stone stone of Bilbo, the Woods of the \n",
            "gones, and stop or this enemy of the \n",
            "thing wolded, as yet very had not \n",
            "hope \n",
            "but no carred this powelf. And looked or g \n",
            "\n",
            " ra\n",
            " ran \n",
            "morting \n",
            "and for a \n",
            "little \n",
            "ones, and go of a stream. \n",
            "\n",
            "The east on aster you had go to rain in for the roaf \n",
            "saw, \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "to Frodo, \n",
            "the door, and the wall time you are surves you his green.' \n",
            "\n",
            "'What \n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "  start_strings = [\" Th\", \" wh\", \" he\", \" I \", \" ca\", \" G\", \" lo\", \" ra\"]\n",
        "  start = random.randint(0,len(start_strings)-1)\n",
        "  print(start_strings[start])\n",
        "#   all_characters.index(string[c])\n",
        "  print(evaluate(start_strings[start], 200), '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJhgDc2IauPE"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 6: Generate output on a different dataset\n",
        "\n",
        "---\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "* Choose a textual dataset. Here are some [text datasets](https://www.kaggle.com/datasets?tags=14104-text+data%2C13205-text+mining) from Kaggle \n",
        "\n",
        "* Generate some decent looking results and evaluate your model's performance (say what it did well / not so well)\n",
        "\n",
        "**DONE:**\n",
        "\n",
        "For my model, the text's grammatical structure and word choice did look like Shakespeare, so that went well. However, most of the words looked made up (but Shakespeare also made up a ton of words, so maybe my model did we\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_characters = string.printable\n",
        "n_characters = len(all_characters)\n",
        "file = unidecode.unidecode(open('./text_files/tiny_shakespeare.txt').read())\n",
        "file_len = len(file)\n",
        "print('file_len =', file_len)\n",
        "\n",
        "n_epochs = 2001\n",
        "print_every = 200\n",
        "plot_every = 10\n",
        "hidden_size = 200\n",
        "n_layers = 3\n",
        "lr = 0.001\n",
        " \n",
        "decoder = RNN(n_characters, hidden_size, n_characters, n_layers).cuda()\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        " \n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  loss_ = train(*random_training_set())       \n",
        "  loss_avg += loss_\n",
        "\n",
        "  if epoch % print_every == 0:\n",
        "      print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n",
        "      print(evaluate('Wh', 100), '\\n')\n",
        "\n",
        "  if epoch % plot_every == 0:\n",
        "      all_losses.append(loss_avg / plot_every)\n",
        "      loss_avg = 0\n",
        "\n",
        "for i in range(10):\n",
        "  start_strings = [\" Th\", \" wh\", \" he\", \" I \", \" ca\", \" G\", \" lo\", \" ra\"]\n",
        "  start = random.randint(0,len(start_strings)-1)\n",
        "  print(start_strings[start])\n",
        "#   all_characters.index(string[c])\n",
        "  print(evaluate(start_strings[start], 200), '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDlSt3H1LsM9",
        "outputId": "f9ab4ec0-fe02-49c2-bf30-2ecad46ac25f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file_len = 1115394\n",
            "[83.51100468635559 (200 9%) 513.7593]\n",
            "Whe thercok toe the ,ar inn indd gher besukey Ie oive\n",
            "\n",
            "AOCHOOARHou holide fare tecath be you bethof, b \n",
            "\n",
            "[157.18163514137268 (400 19%) 425.1576]\n",
            "Which to bape\n",
            "I nom crour wear,\n",
            "Rit co! 'he ur a me wind he cowd hit wasd the: me I wre muon end thes  \n",
            "\n",
            "[230.6080183982849 (600 29%) 400.8165]\n",
            "Wher that to deathouch\n",
            "Come peer to for is she paring coming eter in eir she the ewerge! wher oll id a \n",
            "\n",
            "[303.8841726779938 (800 39%) 349.2711]\n",
            "Wher woor me dears a will for the more.\n",
            "\n",
            "Ceran;\n",
            "So neath our wiwe te me in the he, and it stain.\n",
            "\n",
            "CEMH \n",
            "\n",
            "[377.43756222724915 (1000 49%) 360.2930]\n",
            "Whine my reedt comnord.\n",
            "\n",
            "BRUCCALEN:\n",
            "And sent for so to call Ballic! And beart!\n",
            "\n",
            "Lord: brick my strave. \n",
            "\n",
            "[450.52568078041077 (1200 59%) 325.8656]\n",
            "Whed greak slainiress thing\n",
            "My woos wist, that wen this from here lees the taughtegor,\n",
            "There you then, \n",
            "\n",
            "[523.3996367454529 (1400 69%) 307.7434]\n",
            "Where thou he were,\n",
            "For be, rouds him beart tood head not griend.\n",
            "\n",
            "BANCENTIO:\n",
            "Enday lond the are the g \n",
            "\n",
            "[596.5121212005615 (1600 79%) 353.0759]\n",
            "What's,\n",
            "For for were of her his daw a dive and the inglain,\n",
            "For thy blook and in this heart him to be  \n",
            "\n",
            "[669.8564019203186 (1800 89%) 353.2508]\n",
            "Which thy grovidive then\n",
            "Contensels a traous he goge as the Taclmion.\n",
            "But the count to-greses in thou  \n",
            "\n",
            "[742.7406573295593 (2000 99%) 346.6657]\n",
            "When leave better,\n",
            "Thought a daughtion men\n",
            "We folming of my leive up;\n",
            "For no more mon't it a proved ma \n",
            "\n",
            " lo\n",
            " love good fill.\n",
            "\n",
            "Romer:\n",
            "Fall the seech it my father sport,\n",
            "For the pleaded no more such my lord,\n",
            "Or him your appents the sufper but sore,\n",
            "But deep the like no mage to his resolvet:\n",
            "And will hold be senc \n",
            "\n",
            " he\n",
            " her; I now, I will known\n",
            "It not the love,\n",
            "In our for that good from thoughts; bet my such have\n",
            "The fariel, good, are the stand on perpent end.\n",
            "\n",
            "MENENIUS:\n",
            "Well here: fall me you lordd.\n",
            "\n",
            "CARINA:\n",
            "Mows impr \n",
            "\n",
            " he\n",
            " he be quister, to the booker:\n",
            "be eye will bless and belide the heaven,\n",
            "Thou due your should have not like to the very\n",
            "Deep a fay's wift's leave in my old\n",
            "kn'mby do alout and a tnishold and\n",
            "which ard bel \n",
            "\n",
            " G\n",
            " Give which then belour:\n",
            "Are knowleven the quor say, a put meet he.\n",
            "\n",
            "COMINGBROKE:\n",
            "We sour, may, you art shall not a preak up, pessent not more\n",
            "Lord this not you blood made sweet forer.\n",
            "\n",
            "First:\n",
            "And deser \n",
            "\n",
            " he\n",
            " he tord life,\n",
            "And the pressed stand abany's.\n",
            "\n",
            "GOY ELIZABELLO:\n",
            "Come, no, are heard deither, it good a sig\n",
            "tend it jay dooss lie, I hold and us here;\n",
            "O, sen the witle should be strangain;\n",
            "Who' not the bai \n",
            "\n",
            " Th\n",
            " Thout on wife other,\n",
            "My lord, this sun it propition'd me the rever\n",
            "Didst be she blood one son, and here, slave it our feath\n",
            "then; the old sked and that what are on come, your the pail.\n",
            "\n",
            "Sir:O\n",
            "And not be \n",
            "\n",
            " ra\n",
            " raye our more\n",
            "is. I that it no liest a woll.\n",
            "\n",
            "ISABELLA:\n",
            "I the alous thinks have death.\n",
            "\n",
            "HURSIA:\n",
            "Your hard, preak\n",
            "For my halk! I presences the reder's know free\n",
            "What my lay blood's presseid.\n",
            "Why our spee \n",
            "\n",
            " ca\n",
            " care he will no will have.\n",
            "\n",
            "MORCENTES:\n",
            "I lather sug obeit!\n",
            "\n",
            "ANGEL:\n",
            "I more me here, 'tis foll'd old of thank\n",
            "I'll more a cumpection and the now the his words\n",
            "And this faults though of a save not let for  \n",
            "\n",
            " I \n",
            " I mave wither depence,\n",
            "On ass your confected our puide; and fire,\n",
            "For the poin of bele, as go't be him\n",
            "Uncall I hather, for her. Ye will the pate forthens\n",
            "That's to be provest is to more.\n",
            "\n",
            "SICINIUS:\n",
            "Why \n",
            "\n",
            " G\n",
            " God, let you rope,\n",
            "His pate not for and young all itselven,\n",
            "For so ever meave from prive but me mere and a mine my poor.\n",
            "So for roal, my may know, my,\n",
            "And thou ars like for betwife and will men price i \n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "lab6.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}