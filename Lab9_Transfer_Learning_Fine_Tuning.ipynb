{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEwpv0GZ_CrT"
      },
      "source": [
        "<a \n",
        "href=\"https://colab.research.google.com/github/wingated/cs474_labs_f2019/blob/master/DL_Lab10.ipynb\"\n",
        "  target=\"_parent\">\n",
        "  <img\n",
        "    src=\"https://colab.research.google.com/assets/colab-badge.svg\"\n",
        "    alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnoEhAVvBcMj"
      },
      "source": [
        "#Lab 9: Transfer Learning/Fine-Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBOvJdJfkXIL"
      },
      "source": [
        "## Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiuvTUWOjtBC"
      },
      "source": [
        "### Objective\n",
        "\n",
        "- Gain experience fine-tuning pre-trained models to domain-specific applications.\n",
        "\n",
        "### Deliverable\n",
        "\n",
        "For this lab you will submit an ipython notebook via learning suite. The bulk of the work is in modifying fine-tuning a pre-trained ResNet. Fine-tuning the GPT-2 language model is pretty easy. The provided code works as is; you will just have to swap in your own text dataset.\n",
        "\n",
        "### Grading\n",
        "\n",
        "- 35% Create a dataset class for your own dataset\n",
        "- 35% Create a network class that wraps a pretrained ResNet\n",
        "- 20% Implement unfreezing in the network class\n",
        "- 10% Fine-tune GPT-2 on your own dataset\n",
        "\n",
        "### Tips\n",
        "- Your life will be better if you download a dataset that already has the data in the expected format for ImageFolder (make sure to read the documentation!). The datasets recommended below are in the correct format.\n",
        "- Get the CNN working on the provided dataset (bird species classification) before swapping in your own.\n",
        "- For reference on freezing/unfreezing network weights, see [this github gist](https://gist.github.com/jcjohnson/6e41e8512c17eae5da50aebef3378a4c)\n",
        "- For training GPT-2, first try the medium-size (355M parameter) model. If your Colab instance doesn't have enough GPU space, you may need to switch to the small-size (124M parameter) model, but the results will be less impressive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKzRORuLBNLR"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import resnet152\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "from torch import optim, nn\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "import os\n",
        "import sys\n",
        "from PIL import Image, ImageOps\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4R3D8Mr8b54"
      },
      "source": [
        "## 1 Fine-tune a ResNet for image classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFoEeTYHDq2s"
      },
      "source": [
        "### 1.1 Find a dataset to fine-tune on, and make a Dataset class (1 hr.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z6g7a_Y84n0"
      },
      "source": [
        "#### TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8NtFZRd5hcm"
      },
      "source": [
        "- Inherit from torch.utils.data.Dataset\n",
        "- Use a [torchvision.datasets.ImageFolder](https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.ImageFolder)\n",
        "- Don't spend too long finding another dataset. Some suggestions that you are free to use:\n",
        " - https://www.kaggle.com/akash2907/bird-species-classification\n",
        " - https://www.kaggle.com/jessicali9530/stanford-dogs-dataset\n",
        " - https://www.kaggle.com/puneet6060/intel-image-classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBigIUFTukeJ"
      },
      "source": [
        "#### Help for downloading kaggle datasets\n",
        "Downloading Kaggle datasets requires authentication, so you can't just download from a url. Here are some step-by-step instructions of how to get Kaggle datasets in Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_X29UC6CvwfQ"
      },
      "source": [
        "1. Create an API key in Kaggle\n",
        "    - Click on profile photo\n",
        "    - Go to 'My Account'\n",
        "    - Scroll down to the API access section and click \"Create New API Token\"\n",
        "    - `kaggle.json` is now downloaded to your computer\n",
        "\n",
        "2. Upload the API key and install the Kaggle API client by running the next cell (run it again if it throws an error the first time). Also, `files.upload()` may not work in Firefox. One solution is to expand the Files banner (indicated by the '>' tab on the left side of the page) and use that to upload the key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          }
        },
        "id": "Mhjc0pM7jOoZ",
        "outputId": "6e13ddc9-722b-46a1-8e5c-d31b3391d8ba"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3065d692-f74c-4d43-a85a-087c18da07b1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3065d692-f74c-4d43-a85a-087c18da07b1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "-rw-r--r-- 1 root root 68 Mar  1 19:35 kaggle.json\n"
          ]
        }
      ],
      "source": [
        "# Run this cell and select the kaggle.json file downloaded\n",
        "# from the Kaggle account settings page.\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "# Next, install the Kaggle API client.\n",
        "!pip install -q kaggle\n",
        "# Let's make sure the kaggle.json file is present.\n",
        "!ls -lha kaggle.json\n",
        "# The Kaggle API client expects this file to be in ~/.kaggle,\n",
        "# so move it there.\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "# This permissions change avoids a warning on Kaggle tool startup.\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGlIa4SIwEXB"
      },
      "source": [
        "3. Copy the desired dataset locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HtB-XdIr1EE",
        "outputId": "4c0d78db-79b2-47c2-d19a-4455cdeb5015"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading bird-species-classification.zip to /content\n",
            " 99% 1.36G/1.37G [00:45<00:00, 36.0MB/s]\n",
            "100% 1.37G/1.37G [00:45<00:00, 32.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Example download command for dataset found here: https://www.kaggle.com/akash2907/bird-species-classification\n",
        "!kaggle datasets download -d akash2907/bird-species-classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcz0JGXjxFGe"
      },
      "source": [
        "#### Make the Dataset class\n",
        "See the implementation below for reference, and make your own."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lthPlsGeK4CX"
      },
      "outputs": [],
      "source": [
        "class BirdDataset(Dataset):\n",
        "    def __init__(self, zip_file='./bird-species-classification.zip', size=256, train=True, upload=False):\n",
        "        super(BirdDataset, self).__init__()\n",
        "        \n",
        "        self.train = train\n",
        "        extract_dir = os.path.splitext(zip_file)[0]\n",
        "        if not os.path.exists(extract_dir):\n",
        "            os.makedirs(extract_dir)\n",
        "            self.extract_zip(zip_file, extract_dir)\n",
        "            # Resize the images - originally they are high resolution. We could do this\n",
        "            # in the DataLoader, but it will read the full-resolution files from disk\n",
        "            # every time before resizing them, making training slow\n",
        "            self.resize(extract_dir, size=size)\n",
        "\n",
        "        postfix = 'train' if train else 'test'\n",
        "            \n",
        "        if train:\n",
        "            # The bird-species dataset mistakenly has a train_data folder inside of train_data\n",
        "            self.dataset_folder = datasets.ImageFolder(os.path.join(extract_dir, 'train_data', 'train_data'), transform=transforms.Compose([transforms.ToTensor()]))\n",
        "        else:\n",
        "            self.dataset_folder = datasets.ImageFolder(os.path.join(extract_dir, 'test_data', 'test_data'), transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "    def extract_zip(self, zip_file, extract_dir):\n",
        "        print(\"Extracting\", zip_file)\n",
        "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_dir)\n",
        "\n",
        "    def resize(self, path, size=256):\n",
        "        \"\"\"Resizes all images in place\"\"\"\n",
        "        print(\"Resizing images\")\n",
        "        dirs = os.walk(path)\n",
        "        for root, dirs, files in os.walk(path):\n",
        "            for item in files:\n",
        "                name = os.path.join(root, item)\n",
        "                if os.path.isfile(name):\n",
        "                    im = Image.open(name)\n",
        "                    im = ImageOps.fit(im, (size, size))\n",
        "                    im.save(name[:-3] + 'bmp', 'BMP')\n",
        "                    os.remove(name)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.dataset_folder[i]\n",
        "\n",
        "    def __len__(self):\n",
        "      return 1000\n",
        "        # return len(self.dataset_folder)\n",
        "\n",
        "bird_data = BirdDataset()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d puneet6060/intel-image-classification"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RM6RznTiK2Fh",
        "outputId": "e53fc9d5-7bad-4b93-cfa8-3a39ff5f4abc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading intel-image-classification.zip to /content\n",
            " 97% 337M/346M [00:11<00:00, 24.6MB/s]\n",
            "100% 346M/346M [00:11<00:00, 32.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jHFdToeDtIF"
      },
      "outputs": [],
      "source": [
        "##############################\n",
        "# Implement your own Dataset #\n",
        "##############################\n",
        "class IntelDataset(Dataset):\n",
        "    def __init__(self, zip_file='./intel-image-classification.zip', size=64, train=True, upload=False):\n",
        "        super(IntelDataset, self).__init__()\n",
        "        \n",
        "        self.train = train\n",
        "        extract_dir = os.path.splitext(zip_file)[0]\n",
        "        if not os.path.exists(extract_dir):\n",
        "            os.makedirs(extract_dir)\n",
        "            self.extract_zip(zip_file, extract_dir)\n",
        "            # Resize the images - originally they are high resolution. We could do this\n",
        "            # in the DataLoader, but it will read the full-resolution files from disk\n",
        "            # every time before resizing them, making training slow\n",
        "            self.resize(extract_dir, size=size)\n",
        "\n",
        "        postfix = 'train' if train else 'test'\n",
        "            \n",
        "        if train:\n",
        "            # The bird-species dataset mistakenly has a train_data folder inside of train_data\n",
        "            self.dataset_folder = datasets.ImageFolder(os.path.join(extract_dir, 'seg_train', 'seg_train'), transform=transforms.Compose([transforms.ToTensor()]))\n",
        "        else:\n",
        "            self.dataset_folder = datasets.ImageFolder(os.path.join(extract_dir, 'seg_test', 'seg_test'), transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "    def extract_zip(self, zip_file, extract_dir):\n",
        "        print(\"Extracting\", zip_file)\n",
        "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_dir)\n",
        "\n",
        "    def resize(self, path, size=64):\n",
        "        \"\"\"Resizes all images in place\"\"\"\n",
        "        print(\"Resizing images\")\n",
        "        dirs = os.walk(path)\n",
        "        for root, dirs, files in os.walk(path):\n",
        "            for item in files:\n",
        "                name = os.path.join(root, item)\n",
        "                if os.path.isfile(name):\n",
        "                    im = Image.open(name)\n",
        "                    im = ImageOps.fit(im, (size, size))\n",
        "                    im.save(name[:-3] + 'bmp', 'BMP')\n",
        "                    os.remove(name)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.dataset_folder[i]\n",
        "\n",
        "    def __len__(self):\n",
        "      return 500\n",
        "        #return len((self.dataset_folder) // 5)\n",
        "\n",
        "intel_data = IntelDataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vJVbYcAJAf2"
      },
      "source": [
        "### 1.2 Wrap a pretrained ResNet in an `nn.Module` (30 min)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMOzGDND9FD1"
      },
      "source": [
        "#### TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLvmDHbl9IyG"
      },
      "source": [
        "- Make a model class that inherits from `nn.Module`\n",
        "- Wrap a pretrained ResNet and swap out the last layer of that network with a layer that maps to the number of classes in your new dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOtl8z8G9wbr"
      },
      "source": [
        "#### Make your model class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AY-XU4Mwas0j"
      },
      "outputs": [],
      "source": [
        "class ResNetIntel(nn.Module):\n",
        "    def __init__(self, num_classes, start_frozen=False):\n",
        "        super(ResNetIntel, self).__init__()\n",
        "\n",
        "        # Part 1.2\n",
        "        # Load the model - make sure it is pre-trained\n",
        "        self.resnet_model = resnet152(pretrained=True)\n",
        "\n",
        "        # Part 1.4\n",
        "        if start_frozen:\n",
        "            # Turn off all gradients of the resnet\n",
        "            for p in self.resnet_model.parameters():\n",
        "              p.requires_grad = False\n",
        "        \n",
        "        # Part 1.2\n",
        "        # Look at the code of torchvision.models.resnet152 to find the name of the attribute to override (the last layer of the resnet)\n",
        "        # Override the last layer of the neural network to map to the correct number of classes. Note that this new layer has requires_grad = True\n",
        "        # It is called fc\n",
        "        self.resnet_model.fc = nn.Linear(self.resnet_model.fc.in_features, num_classes)\n",
        "        \n",
        "    def unfreeze(self, n_layers):\n",
        "        # Part 1.4\n",
        "        # Turn on gradients for the last n_layers\n",
        "        params = list(self.resnet_model.parameters())\n",
        "        # Do last n_layers, not n_layers onward.\n",
        "        param = params[len(params) - n_layers:]\n",
        "        for p in param:\n",
        "          p.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Part 1.2\n",
        "        # Pass x through the resnet\n",
        "        return self.resnet_model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Krh0eYy18R9"
      },
      "source": [
        "### 1.3 Read through and run this training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOGrrw2gbIPf"
      },
      "outputs": [],
      "source": [
        "def accuracy(y_hat, y_truth):\n",
        "    \"\"\"Gets average accuracy of a vector of predictions\"\"\"\n",
        "    \n",
        "    preds = torch.argmax(y_hat, dim=1)\n",
        "    acc = torch.mean((preds == y_truth).float())\n",
        "    return acc\n",
        "\n",
        "def evaluate(model, objective, val_loader, device):\n",
        "    \"\"\"Gets average accuracy and loss for the validation set\"\"\"\n",
        "\n",
        "    val_losses = []\n",
        "    val_accs = []\n",
        "    # model.eval() so that batchnorm and dropout work in eval mode\n",
        "    model.eval()\n",
        "    # torch.no_grad() to turn off computation graph creation. This allows for temporal\n",
        "    # and spatial complexity improvements, which allows for larger validation batch \n",
        "    # sizes so it’s recommended\n",
        "    with torch.no_grad():\n",
        "        for x, y_truth in val_loader:\n",
        "\n",
        "            x, y_truth = x.to(device), y_truth.to(device)\n",
        "            y_hat = model(x)\n",
        "            val_loss = objective(y_hat, y_truth)\n",
        "            val_acc = accuracy(y_hat, y_truth)\n",
        "\n",
        "            val_losses.append(val_loss.item())\n",
        "            val_accs.append(val_acc)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    return torch.mean(torch.Tensor(val_losses)), torch.mean(torch.Tensor(val_accs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKESMcKi2E_f"
      },
      "outputs": [],
      "source": [
        "def train(start_frozen=False, model_unfreeze=0):\n",
        "    \"\"\"Fine-tunes a CNN\n",
        "    Args:\n",
        "        start_frozen (bool): whether to start with the network weights frozen.\n",
        "        model_unfreeze (int): the maximum number of network layers to unfreeze\n",
        "    \"\"\"\n",
        "    epochs = 20\n",
        "    # Start with a very low learning rate\n",
        "    lr = .00005\n",
        "    val_every = 3\n",
        "    num_classes = 16\n",
        "    batch_size = 32\n",
        "    device = torch.device('cuda:0')\n",
        "\n",
        "    # Data\n",
        "    train_dataset = IntelDataset(upload=True, train=True)\n",
        "    val_dataset = IntelDataset(upload=True, train=False)\n",
        "    train_loader = DataLoader(train_dataset,\n",
        "                              shuffle=True,\n",
        "                              num_workers=8,\n",
        "                              batch_size=batch_size)\n",
        "    val_loader = DataLoader(val_dataset,\n",
        "                              shuffle=True,\n",
        "                              num_workers=8,\n",
        "                              batch_size=batch_size)\n",
        "    \n",
        "    # Model\n",
        "    model = ResNetIntel(num_classes, start_frozen=start_frozen).to(device)\n",
        "    \n",
        "    # Objective\n",
        "    objective = nn.CrossEntropyLoss()\n",
        "    # Optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-1)\n",
        "\n",
        "    # Progress bar\n",
        "    pbar = tqdm(total=len(train_loader) * epochs)\n",
        "\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    val_losses = []\n",
        "    val_accs = []\n",
        "    \n",
        "    cnt = 0\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # Implement model unfreezing\n",
        "        if epoch < model_unfreeze:\n",
        "            # Part 1.4\n",
        "            # Unfreeze the last layers, one more each epoch\n",
        "            model.unfreeze(epoch+1)\n",
        "        \n",
        "        for x, y_truth in train_loader:\n",
        "        \n",
        "            x, y_truth = x.to(device), y_truth.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            y_hat = model(x)\n",
        "            train_loss = objective(y_hat, y_truth)\n",
        "            train_acc = accuracy(y_hat, y_truth)\n",
        "\n",
        "            train_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_accs.append(train_acc.item())\n",
        "            train_losses.append(train_loss.item())\n",
        "\n",
        "            if cnt % val_every == 0:\n",
        "                val_loss, val_acc = evaluate(model, objective, val_loader, device)\n",
        "                val_losses.append(val_loss.item())\n",
        "                val_accs.append(val_acc.item())\n",
        "\n",
        "            pbar.set_description('train loss:{:.4f}, train accuracy:{:.4f}.'.format(train_loss.item(), train_acc))\n",
        "            pbar.update(1)\n",
        "            cnt += 1\n",
        "\n",
        "    pbar.close()\n",
        "    plt.subplot(121)\n",
        "    plt.plot(np.arange(len(train_accs)), train_accs, label='Train Accuracy')\n",
        "    plt.plot(np.arange(len(train_accs), step=val_every), val_accs, label='Val Accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(122)\n",
        "    plt.plot(np.arange(len(train_losses)), train_losses, label='Train Loss')\n",
        "    plt.plot(np.arange(len(train_losses), step=val_every), val_losses, label='Val Loss')\n",
        "    plt.legend()\n",
        "    plt.show()  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvnxeLotchiH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "86c2bba7-0a57-448c-c9ba-e3b0f886af24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "train loss:0.0097, train accuracy:1.0000.: 100%|██████████| 320/320 [33:03<00:00,  6.20s/it]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "train(start_frozen=False, model_unfreeze=8)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEDv_-H7BvM0"
      },
      "source": [
        "### 1.4 Implement Unfreezing (1 hr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YH5mQBaa-_0b"
      },
      "source": [
        "#### Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_YmE1pe-6LF"
      },
      "source": [
        "Unfreezing is a technique that can be helpful when fine tuning a CNN for a more difficult task with a large amount of data.\n",
        "\n",
        "The idea is that if we allow the network to tweak the earliest layers immediately, before the last FCL has been trained at all, the earliest layers will forget all of the useful features that they learned in order  to provide features that are helpful for the (untrained) FCL.\n",
        "\n",
        "So, rather than training all of the model weights at once, we learn the last fully connected layer, then train that layer together with the second-to-last layer, gradually adding layers until we reach the first layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMKRI77_-8nc"
      },
      "source": [
        "#### TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaUc8BTYC1bz"
      },
      "source": [
        "- Modify your model class by setting the `requires_grad` attribute of the ResNet to `False`. (but keep `requires_grad = True` for the last layer).\n",
        "- Add a member function to you model class that allows the user to unfreeze weights in the training loop. See [this github gist](https://gist.github.com/jcjohnson/6e41e8512c17eae5da50aebef3378a4c) for reference.\n",
        "- Modify your training loop to add logic that calls the `unfreeze` function of the model class (unfreeze one layer every epoch).\n",
        "- Call your train function to fine-tune the ResNet on your dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBT5jgifC7Im"
      },
      "source": [
        "#### Call your train function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mg9ySEO_BNDx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "85fdf808-865b-4781-d1c4-6621d9da4001"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "train loss:0.0176, train accuracy:1.0000.: 100%|██████████| 320/320 [25:05<00:00,  4.70s/it]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "###############################################################################\n",
        "# train with unfreezing here (should be a single call to your train function) #\n",
        "###############################################################################\n",
        "train(start_frozen=True, model_unfreeze=6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZItH2lX7k4Yt"
      },
      "source": [
        "You may not see any improvement for your classification task, but unfreezing can help convergence for more difficult image classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAXHAUf3EEiE"
      },
      "source": [
        "##2 Fine-tune a language model - (15 min)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yu9usOxtjFHL"
      },
      "source": [
        "In this section you will use the gpt-2-simple package [here](https://github.com/minimaxir/gpt-2-simple) to fine-tune the GPT-2 language model on a domain of your choice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K7F19SPQo6U"
      },
      "source": [
        "### 2.1 Generate text from the pretrained GPT-2 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YLXvK51RnuL"
      },
      "source": [
        "#### Run this code to generate text from a pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDNOb_H5IRvH",
        "outputId": "091bed54-3275-4e44-f959-c31d0bed6791"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gpt-2-simple in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: toposort in /usr/local/lib/python3.7/dist-packages (from gpt-2-simple) (1.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gpt-2-simple) (4.62.3)\n",
            "Requirement already satisfied: tensorflow>=2.5.1 in /usr/local/lib/python3.7/dist-packages (from gpt-2-simple) (2.8.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from gpt-2-simple) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from gpt-2-simple) (1.21.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gpt-2-simple) (2.23.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.5.3)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.8.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.44.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (57.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.6.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.13.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.2.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.1.2)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (13.0.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.10.0.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.24.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.15.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.8.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow>=2.5.1->gpt-2-simple) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.5.1->gpt-2-simple) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.5.1->gpt-2-simple) (3.3.6)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.5.1->gpt-2-simple) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.5.1->gpt-2-simple) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.5.1->gpt-2-simple) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.5.1->gpt-2-simple) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.5.1->gpt-2-simple) (1.8.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.5.1->gpt-2-simple) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.5.1->gpt-2-simple) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.5.1->gpt-2-simple) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.5.1->gpt-2-simple) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.5.1->gpt-2-simple) (4.11.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.5.1->gpt-2-simple) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.5.1->gpt-2-simple) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gpt-2-simple) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gpt-2-simple) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gpt-2-simple) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gpt-2-simple) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.5.1->gpt-2-simple) (3.2.0)\n",
            "TensorFlow 1.x selected.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install gpt-2-simple\n",
        "\n",
        "# the transformers package is built on top of Tensorflow, and the default TF version \n",
        "# for Colab will soon switch to 2.x. We remedy this with the following magic method\n",
        "%tensorflow_version 1.x \n",
        "\n",
        "import gpt_2_simple as gpt2\n",
        "import os\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aRJ-c9uRMOa",
        "outputId": "2a9fa36a-82a7-40fa-a5d9-601cc5abff45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained model models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n",
            "I tried to do my bit to help him with the situation, and then I realized he gave me a few more minutes to figure something out. I decided to show him the full team sitting side by side and the one who is in this chat (I think) is likely to be his dad.\n",
            "\n",
            "I also made sure he was aware of my actions. I think he's a little bit more fluid than I thought. I know he isn't the type to take things personally and that he doesn't know what to do. He is just that kind of guy that is nice to talk to and explain to and say good bye.\n",
            "\n",
            "Speaking of Good bye, I'm going to be able to do some other stuff and help him out a bit. I need to get into some more of my talking to him more, because he's needed a lot of time. I know he's the kind of guy that will be helped by someone. I could do a lot of community event on my own, but I have to do it the right way. I just need to be able to help him out a bit.\n",
            "\n",
            "I think we were just having a good time. I don't know how to describe it, but Good bye to everyone.\n",
            "\n",
            "I'm going to start the game with him and try to keep him on the field with me. I want him to be a big day player. I'm going to try to get him to have a good day. I think he's going to get a lot of hits and he's going to run out of time.\n",
            "\n",
            "I can play with him, I can run him up, I can run him down and I can get him to run. I can run him through a tough situation. I know there will be some negative consequences if I'm not able to play the right way. I know that I can hurt him and that's why I'm doing this.\n",
            "\n",
            "Good bye to all the community. I'm going to give him a great day. It's been a good day for him.<|endoftext|>Cease and desist from using the FOG or FOGS.\n",
            "\n",
            "The FOGS is the largest and most powerful FOG synth to date.\n",
            "\n",
            "FOGS is a custom built FOG synth with a 6-axis single-axis oscillator.\n",
            "\n",
            "The FOGS is a bolt-on, built to order, FOGS-exclusive, and open source FOG synth.\n",
            "\n",
            "The FOGS is available in three versions:\n",
            "\n",
            "Classic FOG synth with 12.5\" FOG input (12.5mm in circumference)\n",
            "\n",
            "FOG-FOG-PERF synth with 16\" FOG input (16mm in circumference)\n",
            "\n",
            "Classic FOG synth with 16\" FOG input (16mm in circumference)\n",
            "\n",
            "FOG-FOG-PERF synth with 16\" FOG input (16mm in circumference)\n",
            "\n",
            "Simple FOG synth with built-in FOG-PWM control (32\" FOG input, 24\" FOG output)\n",
            "\n",
            "FOG-FOG-PERF synth with 16\" FOG input (16mm in circumference)\n",
            "\n",
            "Final FOG synth (the original)\n",
            "\n",
            "FOG-FOG-PERF synth with 16\" FOG input (16mm in circumference)\n",
            "\n",
            "FOG-FOG-PERF synth with 16\" FOG input (16mm in circumference)\n",
            "\n",
            "FOG-FOG-PERF synth with 16\" FOG input (16mm in circumference)\n",
            "\n",
            "FOG-FOG-PERF synth with 16\" FOG input (16mm in circumference)\n",
            "\n",
            "FOG-FOG-PERF synth with 16\" FOG input (16mm in circumference)\n",
            "\n",
            "FOG-FOG-PERF synth with 16\" FOG input (16mm in circumference)\n",
            "\n",
            "FOG-FOG-PERF synth with 16\" FOG input (16mm in circumference)\n",
            "\n",
            "FOG-FOG-PERF synth with 16\" FOG input (16mm in circumference)\n",
            "\n",
            "FOG-FOG-PERF synth with 16\" FOG input (16mm in circumference)\n",
            "\n",
            "FOG-FOG-PERF synth with 16\" FOG input (16mm in circumference)\n",
            "\n",
            "FOG-FOG-PERF synth with 16\" FOG input (16mm in circumference)\n",
            "\n",
            "FOG-FOG-PERF synth with 16\" FOG input (16mm in circumference)\n",
            "\n",
            "FOG-FOG-PERF synth with 16\" FOG input (16mm in circumference)\n",
            "\n",
            "FOG-FOG-PERF synth with 16\" FOG input (16mm in circumference)\n",
            "\n",
            "FOG-FOG-PERF synth with 16\" FOG input (16mm in circumference)\n",
            "\n",
            "FOG-FOG-PERF synth with\n"
          ]
        }
      ],
      "source": [
        "# This line is necessary to be able to run a new tf session\n",
        "tf.reset_default_graph()\n",
        "# The medium-sized model. IF you run out of memory, try \"124M\" instead\n",
        "model_name = \"124M\"\n",
        "if not os.path.isdir(os.path.join(\"models\", model_name)):\n",
        "\tprint(f\"Downloading {model_name} model...\")\n",
        "\tgpt2.download_gpt2(model_name=model_name)   # model is saved into current directory under /models/124M/\n",
        "\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, model_name=model_name)\n",
        "gpt2.generate(sess, model_name=model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHmjSVf_FNHv"
      },
      "source": [
        "### 2.2 Download a text dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPXJkNubFyY6"
      },
      "source": [
        "#### TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWkuRjbcFzwb"
      },
      "source": [
        "- Use the provided functions to download your own text dataset\n",
        "- [Project Gutenberg](https://www.gutenberg.org/) is a nice starting point for raw text corpora"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD45m3IwF9hh"
      },
      "source": [
        "#### Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESltl2QM5nxw",
        "outputId": "865d6b14-cb4e-48bc-9f79-53b78c04a0c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat '/data/text/996-0.txt': No such file or directory\n",
            "996-h\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "from torchvision import datasets\n",
        "\n",
        "def extract_zip(zip_path, remove_finished=True):\n",
        "    print('Extracting {}'.format(zip_path))\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(zip_path.replace('.zip', ''))\n",
        "    if remove_finished:\n",
        "        os.remove(zip_path)\n",
        "\n",
        "def download_dataset(url, root='../data'):\n",
        "    if not os.path.exists(os.path.join(root, 'text')):\n",
        "        os.makedirs(os.path.join(root))\n",
        "        datasets.utils.download_url(url, root, 'text.zip', None)\n",
        "        extract_zip(os.path.join(root, 'text.zip'))\n",
        "    return os.path.join(root, 'text')\n",
        "\n",
        "##########################################\n",
        "# Set the url for your dataset here,\n",
        "# move the dataset to the desired location\n",
        "##########################################\n",
        "#url = 'https://www.gutenberg.org/files/30/30.zip'\n",
        "url = 'https://www.gutenberg.org/files/996/996-0.zip'\n",
        "download_dataset(url)\n",
        "!mv /data/text/996-0.txt /data/text/donq.txt\n",
        "!ls ../data/text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://www.gutenberg.org/files/996/996-0.txt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6-sZKsNgAMV",
        "outputId": "3ca6f3a7-59c2-4fc1-dbc0-f24a4b023517"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-01 22:01:14--  https://www.gutenberg.org/files/996/996-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2391541 (2.3M) [text/plain]\n",
            "Saving to: ‘996-0.txt’\n",
            "\n",
            "996-0.txt           100%[===================>]   2.28M  1.75MB/s    in 1.3s    \n",
            "\n",
            "2022-03-01 22:01:16 (1.75 MB/s) - ‘996-0.txt’ saved [2391541/2391541]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usQE-rSPZq_X"
      },
      "source": [
        "### 2.3 Fine-tune GPT-2 on your own dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoA0tZZCa_1k"
      },
      "source": [
        "#### TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoU6ML1mbgjP"
      },
      "source": [
        "- Swap out the dataset parameter with the path to your dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pa5vFJ5EUjv"
      },
      "source": [
        "#### Train on your dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuQ5snl4LuS0",
        "outputId": "d2325ba7-a658-40bc-ad5c-35cc0f34c664"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Loading checkpoint models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:03<00:00,  3.82s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset has 621198 tokens\n",
            "Training...\n",
            "[1 | 12.18] loss=3.66 avg=3.66\n",
            "[2 | 16.88] loss=3.55 avg=3.60\n",
            "[3 | 21.56] loss=3.38 avg=3.53\n",
            "[4 | 26.24] loss=3.55 avg=3.53\n",
            "[5 | 30.91] loss=3.38 avg=3.50\n",
            "[6 | 35.58] loss=3.57 avg=3.51\n",
            "[7 | 40.25] loss=3.46 avg=3.51\n",
            "[8 | 44.94] loss=3.40 avg=3.49\n",
            "[9 | 49.62] loss=3.27 avg=3.47\n",
            "[10 | 54.33] loss=3.34 avg=3.45\n",
            "[11 | 59.02] loss=3.28 avg=3.44\n",
            "[12 | 63.76] loss=3.38 avg=3.43\n",
            "[13 | 68.47] loss=3.25 avg=3.42\n",
            "[14 | 73.17] loss=3.27 avg=3.41\n",
            "[15 | 77.89] loss=3.37 avg=3.40\n",
            "[16 | 82.60] loss=3.19 avg=3.39\n",
            "[17 | 87.34] loss=3.21 avg=3.38\n",
            "[18 | 92.05] loss=3.20 avg=3.37\n",
            "[19 | 96.77] loss=3.33 avg=3.36\n",
            "[20 | 101.48] loss=3.37 avg=3.37\n",
            "[21 | 106.21] loss=3.07 avg=3.35\n",
            "[22 | 110.93] loss=2.97 avg=3.33\n",
            "[23 | 115.64] loss=3.16 avg=3.32\n",
            "[24 | 120.37] loss=3.27 avg=3.32\n",
            "[25 | 125.07] loss=3.06 avg=3.31\n",
            "[26 | 129.77] loss=3.19 avg=3.30\n",
            "[27 | 134.48] loss=3.12 avg=3.30\n",
            "[28 | 139.19] loss=3.28 avg=3.29\n",
            "[29 | 143.91] loss=3.05 avg=3.29\n",
            "[30 | 148.63] loss=3.11 avg=3.28\n",
            "[31 | 153.35] loss=3.05 avg=3.27\n",
            "[32 | 158.05] loss=3.09 avg=3.26\n",
            "[33 | 162.77] loss=2.93 avg=3.25\n",
            "[34 | 167.49] loss=3.14 avg=3.25\n",
            "[35 | 172.20] loss=3.34 avg=3.25\n",
            "[36 | 176.91] loss=3.12 avg=3.25\n",
            "[37 | 181.61] loss=3.11 avg=3.24\n",
            "[38 | 186.33] loss=3.11 avg=3.24\n",
            "[39 | 191.04] loss=3.22 avg=3.24\n",
            "[40 | 195.74] loss=3.12 avg=3.23\n",
            "[41 | 200.45] loss=3.13 avg=3.23\n",
            "[42 | 205.18] loss=3.12 avg=3.23\n",
            "[43 | 209.86] loss=3.09 avg=3.22\n",
            "[44 | 214.58] loss=3.12 avg=3.22\n",
            "[45 | 219.28] loss=3.16 avg=3.22\n",
            "[46 | 223.99] loss=3.19 avg=3.22\n",
            "[47 | 228.70] loss=3.17 avg=3.22\n",
            "[48 | 233.42] loss=3.07 avg=3.21\n",
            "[49 | 238.14] loss=2.91 avg=3.21\n",
            "[50 | 242.87] loss=3.11 avg=3.20\n",
            "[51 | 247.59] loss=2.98 avg=3.20\n",
            "[52 | 252.30] loss=2.85 avg=3.19\n",
            "[53 | 257.01] loss=2.98 avg=3.18\n",
            "[54 | 261.72] loss=3.05 avg=3.18\n",
            "[55 | 266.44] loss=2.85 avg=3.17\n",
            "[56 | 271.13] loss=2.94 avg=3.17\n",
            "[57 | 275.85] loss=2.73 avg=3.16\n",
            "[58 | 280.56] loss=3.03 avg=3.15\n",
            "[59 | 285.26] loss=2.83 avg=3.15\n",
            "[60 | 289.96] loss=3.00 avg=3.14\n",
            "[61 | 294.67] loss=2.89 avg=3.14\n",
            "[62 | 299.39] loss=2.97 avg=3.14\n",
            "[63 | 304.11] loss=2.95 avg=3.13\n",
            "[64 | 308.83] loss=2.92 avg=3.13\n",
            "[65 | 313.55] loss=3.01 avg=3.12\n",
            "[66 | 318.26] loss=3.03 avg=3.12\n",
            "[67 | 322.98] loss=2.92 avg=3.12\n",
            "[68 | 327.69] loss=2.93 avg=3.11\n",
            "[69 | 332.40] loss=2.94 avg=3.11\n",
            "[70 | 337.09] loss=3.05 avg=3.11\n",
            "[71 | 341.82] loss=3.06 avg=3.11\n",
            "[72 | 346.53] loss=3.01 avg=3.11\n",
            "[73 | 351.26] loss=3.10 avg=3.11\n",
            "[74 | 355.96] loss=3.00 avg=3.10\n",
            "[75 | 360.67] loss=2.88 avg=3.10\n",
            "[76 | 365.39] loss=2.86 avg=3.10\n",
            "[77 | 370.08] loss=3.01 avg=3.09\n",
            "[78 | 374.80] loss=3.04 avg=3.09\n",
            "[79 | 379.52] loss=2.98 avg=3.09\n",
            "[80 | 384.22] loss=2.88 avg=3.09\n",
            "[81 | 388.92] loss=2.88 avg=3.08\n",
            "[82 | 393.64] loss=3.05 avg=3.08\n",
            "[83 | 398.34] loss=2.86 avg=3.08\n",
            "[84 | 403.04] loss=2.94 avg=3.08\n",
            "[85 | 407.74] loss=2.92 avg=3.07\n",
            "[86 | 412.43] loss=2.82 avg=3.07\n",
            "[87 | 417.12] loss=2.90 avg=3.07\n",
            "[88 | 421.83] loss=3.09 avg=3.07\n",
            "[89 | 426.55] loss=3.04 avg=3.07\n",
            "[90 | 431.26] loss=3.21 avg=3.07\n",
            "[91 | 435.94] loss=2.75 avg=3.06\n",
            "[92 | 440.63] loss=2.70 avg=3.06\n",
            "[93 | 445.34] loss=2.91 avg=3.06\n",
            "[94 | 450.05] loss=3.18 avg=3.06\n",
            "[95 | 454.75] loss=2.92 avg=3.05\n",
            "[96 | 459.43] loss=2.91 avg=3.05\n",
            "[97 | 464.13] loss=2.71 avg=3.05\n",
            "[98 | 468.83] loss=2.98 avg=3.05\n",
            "[99 | 473.53] loss=2.98 avg=3.05\n",
            "[100 | 478.24] loss=2.87 avg=3.04\n",
            "======== SAMPLE 1 ========\n",
            " out of his head was like a mountain falling upon the trees; and it struck me, therefore, that when all was thus arranged, he who would have been right was, that he who did not observe the end was only wrong, but that it was the fault of the duke that had done what he ought to have done; and so after this a good many letters passed between us, which led to some confusion and confusion; for I never saw in my own life a person with so much of a temper, if he were a man of any other age, than one of the greatest poets in the world.\n",
            "\n",
            "On the end of these letters the duke asked Sancho to call us all and take the oath, and we could not do so, and so he told us all was done before it reached us; but by saying that what he ought to have said would be said by our duke, he came to the conclusion he needed to say no more: and while we were listening for the result of the oaths, and that we heard from his friend that he had received the oath, the baying ceased, and there we stood, saying,\n",
            "As this was for the oath, I did not take it. The duchess looked him in the eye, and said, “Well then, Sancho, for the good part of two quarters a day do nothing; nor is it advisable to do any other than what a man who has taken these oaths has said; but if the duchess should take a look for the day or night, it would seem to you that I do not care to do anything else, for with what I have told you I may take your word that I am bound by the honour they have bestowed upon me and the promises they make me.” As it happened, the duke came out of the chamber, and he had been in a great hurry, but the duchess asked who I was. She said it was the duke of the island of Marilis; I replied, “The duke of Marilis, or, if you like, he who is to be king of this island.”\n",
            "\n",
            "“All the same,” said she; “then you must say whether I am Sancho Maritornes, or if I am not, which will be the first question, and I will say no more than that I am a Knight of the Round Table, a Knight of the Round Table, and therefore a free man; henceforth, let it as to these two things be that they are, that they are of the Round Table and not of the Parleyage.”\n",
            "\n",
            "“Well then,” said the duchess, “how can he be an enemy, or a friend, or friend of knights-errant or of the Round Table, to whom a certain knight-errant, which we have here, might come out and set himself on the spot?”\n",
            "\n",
            "“He cannot be an enemy,” thought Sancho, since he might go on to say: “and I cannot give any answer to him, but only I promise I will take the oath of my life.” After this he said to his friend, “Thou wilt be on the point of taking an oath; though I know he may be ignorant of the things that are to be offered.’\n",
            "\n",
            "“Then let the knight-errant answer himself,’” said he, and leaving the house he went at half-past two p.m., leaving himself off to be alone.\n",
            "\n",
            "The next morning the duchess came to the conclusion that all of the letters which had been put before us had been given to the duke of the island, and that at the end of the day Sancho would have had no time to leave the house, and he left the duchess and all present with a very good hope that this letter which was given from the duke of the island was indeed the letter promised to keep the government from being taken by force under duress or by any means; for the duke that is the duchess of the island must know the letter and make a resolution to let it be signed from the heart to the hand; the duchess may make it and take it out, for her is all that she is.”\n",
            "\n",
            "\n",
            "\n",
            "p65e.jpg (261K)\n",
            "\n",
            "\n",
            "\n",
            "CHAPTER XXI.\n",
            "OF OUR BODIES DELIVERING OUR LETTERS TO THE GREAT KNIGHT TOGETHER WITH THE BOOKS\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "p66a.jpg (285K)\n",
            "\n",
            "Full Size\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "That night he came home, on the very same day, and the duke gave us the letter that had just come us, and having read it, he told us he had never read it in his life, and he might have written it\n",
            "\n",
            "[101 | 500.75] loss=2.96 avg=3.04\n",
            "[102 | 505.46] loss=2.86 avg=3.04\n",
            "[103 | 510.17] loss=2.98 avg=3.04\n",
            "[104 | 514.87] loss=2.85 avg=3.03\n",
            "[105 | 519.58] loss=2.81 avg=3.03\n",
            "[106 | 524.27] loss=2.98 avg=3.03\n",
            "[107 | 528.96] loss=2.99 avg=3.03\n",
            "[108 | 533.66] loss=3.07 avg=3.03\n",
            "[109 | 538.34] loss=2.77 avg=3.03\n",
            "[110 | 543.04] loss=2.98 avg=3.03\n",
            "[111 | 547.75] loss=2.70 avg=3.02\n",
            "[112 | 552.45] loss=2.93 avg=3.02\n",
            "[113 | 557.15] loss=2.85 avg=3.02\n",
            "[114 | 561.86] loss=3.12 avg=3.02\n",
            "[115 | 566.57] loss=2.91 avg=3.02\n",
            "[116 | 571.26] loss=2.76 avg=3.01\n",
            "[117 | 575.97] loss=2.75 avg=3.01\n",
            "[118 | 580.67] loss=2.99 avg=3.01\n",
            "[119 | 585.37] loss=2.73 avg=3.01\n",
            "[120 | 590.08] loss=2.90 avg=3.00\n",
            "[121 | 594.78] loss=2.72 avg=3.00\n",
            "[122 | 599.49] loss=2.87 avg=3.00\n",
            "[123 | 604.18] loss=3.07 avg=3.00\n",
            "[124 | 608.87] loss=2.86 avg=3.00\n",
            "[125 | 613.60] loss=2.67 avg=2.99\n",
            "[126 | 618.28] loss=2.74 avg=2.99\n",
            "[127 | 622.98] loss=2.99 avg=2.99\n",
            "[128 | 627.70] loss=3.03 avg=2.99\n",
            "[129 | 632.39] loss=2.75 avg=2.99\n",
            "[130 | 637.07] loss=2.91 avg=2.98\n",
            "[131 | 641.78] loss=2.75 avg=2.98\n",
            "[132 | 646.48] loss=2.72 avg=2.98\n",
            "[133 | 651.15] loss=2.85 avg=2.98\n",
            "[134 | 655.85] loss=2.68 avg=2.97\n",
            "[135 | 660.55] loss=3.04 avg=2.97\n",
            "[136 | 665.25] loss=3.00 avg=2.97\n",
            "[137 | 669.94] loss=2.99 avg=2.97\n",
            "[138 | 674.64] loss=2.59 avg=2.97\n",
            "[139 | 679.33] loss=3.11 avg=2.97\n",
            "[140 | 684.04] loss=2.85 avg=2.97\n",
            "[141 | 688.74] loss=2.89 avg=2.97\n",
            "[142 | 693.46] loss=2.82 avg=2.97\n",
            "[143 | 698.16] loss=2.82 avg=2.96\n",
            "[144 | 702.85] loss=2.98 avg=2.96\n",
            "[145 | 707.55] loss=2.86 avg=2.96\n",
            "[146 | 712.25] loss=2.92 avg=2.96\n",
            "[147 | 716.94] loss=2.64 avg=2.96\n",
            "[148 | 721.63] loss=2.87 avg=2.96\n",
            "[149 | 726.34] loss=2.78 avg=2.95\n",
            "[150 | 731.03] loss=2.81 avg=2.95\n",
            "[151 | 735.72] loss=2.66 avg=2.95\n",
            "[152 | 740.41] loss=2.60 avg=2.94\n",
            "[153 | 745.11] loss=2.69 avg=2.94\n",
            "[154 | 749.81] loss=2.66 avg=2.94\n",
            "[155 | 754.52] loss=2.76 avg=2.94\n",
            "[156 | 759.21] loss=2.75 avg=2.93\n",
            "[157 | 763.89] loss=2.89 avg=2.93\n",
            "[158 | 768.56] loss=2.95 avg=2.93\n",
            "[159 | 773.24] loss=2.77 avg=2.93\n",
            "[160 | 777.93] loss=2.82 avg=2.93\n",
            "[161 | 782.62] loss=3.09 avg=2.93\n",
            "[162 | 787.30] loss=2.76 avg=2.93\n",
            "[163 | 791.98] loss=2.76 avg=2.93\n",
            "[164 | 796.68] loss=2.70 avg=2.92\n",
            "[165 | 801.34] loss=2.68 avg=2.92\n",
            "[166 | 806.02] loss=2.95 avg=2.92\n",
            "[167 | 810.68] loss=2.72 avg=2.92\n",
            "[168 | 815.35] loss=2.79 avg=2.92\n",
            "[169 | 820.02] loss=2.61 avg=2.91\n",
            "[170 | 824.71] loss=2.68 avg=2.91\n",
            "[171 | 829.40] loss=2.79 avg=2.91\n",
            "[172 | 834.07] loss=2.81 avg=2.91\n",
            "[173 | 838.76] loss=2.86 avg=2.91\n",
            "[174 | 843.43] loss=2.79 avg=2.91\n",
            "[175 | 848.09] loss=2.78 avg=2.91\n",
            "[176 | 852.77] loss=2.79 avg=2.90\n",
            "[177 | 857.45] loss=2.92 avg=2.90\n",
            "[178 | 862.12] loss=2.57 avg=2.90\n",
            "[179 | 866.82] loss=2.82 avg=2.90\n",
            "[180 | 871.51] loss=2.69 avg=2.90\n",
            "[181 | 876.19] loss=2.79 avg=2.90\n",
            "[182 | 880.89] loss=2.77 avg=2.89\n",
            "[183 | 885.60] loss=3.20 avg=2.90\n",
            "[184 | 890.28] loss=2.85 avg=2.90\n",
            "[185 | 894.98] loss=3.01 avg=2.90\n",
            "[186 | 899.68] loss=3.08 avg=2.90\n",
            "[187 | 904.36] loss=2.88 avg=2.90\n",
            "[188 | 909.07] loss=2.66 avg=2.90\n",
            "[189 | 913.76] loss=2.40 avg=2.89\n",
            "[190 | 918.48] loss=2.89 avg=2.89\n",
            "[191 | 923.17] loss=2.76 avg=2.89\n",
            "[192 | 927.86] loss=2.75 avg=2.89\n",
            "[193 | 932.55] loss=2.95 avg=2.89\n",
            "[194 | 937.23] loss=3.13 avg=2.89\n",
            "[195 | 941.92] loss=2.65 avg=2.89\n",
            "[196 | 946.60] loss=2.98 avg=2.89\n",
            "[197 | 951.29] loss=2.73 avg=2.89\n",
            "[198 | 955.98] loss=2.96 avg=2.89\n",
            "[199 | 960.66] loss=2.70 avg=2.89\n",
            "[200 | 965.34] loss=2.78 avg=2.89\n",
            "======== SAMPLE 1 ========\n",
            "\n",
            "\n",
            "\n",
            "“A knight, I suppose, and perhaps a governor, and not going\n",
            "to court, or even to town-hall, he should take his turn\n",
            "going by horse, and as to all, go all the way by a mule—a mule he\n",
            "can go, that he won’t turn into another, or one he was never\n",
            "ready to mount; as there’s not a one I doubt they ever found,\n",
            "though he’s on his way there; if he could only see he was dead.”\n",
            "\n",
            "The man was, however, quite the opposite, and in fact with an instant,\n",
            "with the head turned he came trod; however he did not, and on seeing himself\n",
            "satisfied with going, he said to the duke:\n",
            "\n",
            "“I have not so far told the whole story, but if the governor’s\n",
            "conclave or the barber’s barber had found me in prison, I’d tell\n",
            "them who found me my father and my mother; and perhaps they would have\n",
            "not killed me, that I would have put them to flight, if that be\n",
            "true; for, it is all right in my case, and it is mine, whether God\n",
            "makes it out to me, or not; and if it be my fault there was not an\n",
            "accountary knight there, though they did not die; it was my father\n",
            "myself at fault, too, and my mother, but that was the\n",
            "good faith of the whole house; for in the first place the town\n",
            "saved me from the galleys, and I am bound to follow the law and\n",
            "the rules of knight-errantry, and at the end of fourteen days go\n",
            "thou wert free; and in other things, I shall not meddle with\n",
            "my neighbours’ names, and I shall be the lord and queen of my\n",
            "land.”\n",
            "\n",
            "“But that’s far from the truth,” said the barber; “you may not do\n",
            "anything in prison that would go over well with the court; you may\n",
            "go back to court all the time and pay the dues, and as to this the\n",
            "judge will find me and take good heed of myself and the whole household;\n",
            "“give me a couple of weeks to pay you.”\n",
            "\n",
            "So saying, the judge let go the duke and entered the yard, which was\n",
            "completely empty, and there were many who entered through a window, as\n",
            "they did as the landlord had done to the others in the yard, which seemed very\n",
            "to them so; and as a matter of fact, the officers of the court, who\n",
            "were all present, having never seen the yard before, had it not been\n",
            "thanked with the cries they said it was full of, and with all\n",
            "the bells of the yard full; who could have heard them?\n",
            "\n",
            "The landlord came running out in a loud voice on foot and on horseback,\n",
            "whom the Judge brought into the yard.\n",
            "\n",
            "“Here,” said the judge, “you have already told me my master is gone;\n",
            "and you cannot find him in his own yard?\n",
            "Look, here is the yard of your honour, not from your master’s house,\n",
            "but from another; and look at what the barber found there. He has\n",
            "found, I say, the door, and the window, and a piece of brass with\n",
            "carved on it; see now, master, the walls come with the hinges or\n",
            "locks; look at the windows—that’s the yard,” said the barber,\n",
            "and in all directions that were visible to him, putting all his efforts\n",
            "into finding him, he said he had found—an impossible thing, no? and the\n",
            "Judge, seeing how he was getting round to it, was a bit of a\n",
            "liver-head out of him, and had no intention of putting him to flight again;\n",
            "and so, seeing all was done with nothing but the master and landlord again\n",
            "going, having seen the yard they came out again, seeing the whole\n",
            "yard full of knights, and not a single one not having an accountary at\n",
            "the door. And so—\n",
            "in a few words—there’s nothing left to hear, and they come\n",
            "off with no accountary but a half-pale wench, where no knight\n",
            "ever stood.\n",
            "\n",
            "\n",
            "\n",
            "p33e.jpg (322K)\n",
            "\n",
            "Full Size\n",
            "\n",
            "\n",
            "\n",
            "And a very great number of the knights left, and the landlord was\n",
            "so filled with despair that he went back to the yard, and in a flash, and only\n",
            "half a league in he saw a couple of them get out of their mules and\n",
            "squires and take the whole field\n",
            "\n",
            "[201 | 986.54] loss=2.69 avg=2.88\n",
            "[202 | 991.23] loss=2.64 avg=2.88\n",
            "[203 | 995.93] loss=2.55 avg=2.88\n",
            "[204 | 1000.62] loss=2.56 avg=2.87\n",
            "[205 | 1005.31] loss=2.85 avg=2.87\n",
            "[206 | 1009.99] loss=2.85 avg=2.87\n",
            "[207 | 1014.68] loss=2.74 avg=2.87\n",
            "[208 | 1019.37] loss=2.84 avg=2.87\n",
            "[209 | 1024.09] loss=2.78 avg=2.87\n",
            "[210 | 1028.78] loss=2.76 avg=2.87\n",
            "[211 | 1033.47] loss=2.74 avg=2.87\n",
            "[212 | 1038.14] loss=2.57 avg=2.86\n",
            "[213 | 1042.84] loss=2.88 avg=2.86\n",
            "[214 | 1047.52] loss=2.75 avg=2.86\n",
            "[215 | 1052.22] loss=2.91 avg=2.86\n",
            "[216 | 1056.90] loss=2.61 avg=2.86\n",
            "[217 | 1061.59] loss=2.61 avg=2.86\n",
            "[218 | 1066.28] loss=2.58 avg=2.85\n",
            "[219 | 1070.94] loss=2.68 avg=2.85\n",
            "[220 | 1075.65] loss=2.79 avg=2.85\n",
            "[221 | 1080.34] loss=2.83 avg=2.85\n",
            "[222 | 1085.05] loss=2.72 avg=2.85\n",
            "[223 | 1089.75] loss=2.59 avg=2.85\n",
            "[224 | 1094.44] loss=2.72 avg=2.85\n",
            "[225 | 1099.15] loss=2.38 avg=2.84\n",
            "[226 | 1103.84] loss=2.60 avg=2.84\n",
            "[227 | 1108.53] loss=2.39 avg=2.83\n",
            "[228 | 1113.23] loss=2.59 avg=2.83\n",
            "[229 | 1117.92] loss=2.60 avg=2.83\n",
            "[230 | 1122.62] loss=3.12 avg=2.83\n",
            "[231 | 1127.32] loss=2.71 avg=2.83\n",
            "[232 | 1132.02] loss=2.88 avg=2.83\n",
            "[233 | 1136.71] loss=2.62 avg=2.83\n",
            "[234 | 1141.41] loss=2.69 avg=2.83\n",
            "[235 | 1146.12] loss=2.81 avg=2.83\n",
            "[236 | 1150.81] loss=2.81 avg=2.83\n",
            "[237 | 1155.49] loss=2.73 avg=2.82\n",
            "[238 | 1160.19] loss=2.44 avg=2.82\n",
            "[239 | 1164.88] loss=2.94 avg=2.82\n",
            "[240 | 1169.58] loss=2.56 avg=2.82\n",
            "[241 | 1174.28] loss=2.76 avg=2.82\n",
            "[242 | 1179.00] loss=2.81 avg=2.82\n",
            "[243 | 1183.68] loss=2.62 avg=2.82\n",
            "[244 | 1188.38] loss=2.85 avg=2.82\n",
            "[245 | 1193.08] loss=2.92 avg=2.82\n",
            "[246 | 1197.75] loss=2.92 avg=2.82\n",
            "[247 | 1202.43] loss=2.48 avg=2.81\n",
            "[248 | 1207.11] loss=2.76 avg=2.81\n",
            "[249 | 1211.82] loss=2.57 avg=2.81\n",
            "[250 | 1216.51] loss=2.79 avg=2.81\n",
            "[251 | 1221.22] loss=2.61 avg=2.81\n",
            "[252 | 1225.93] loss=2.73 avg=2.81\n",
            "[253 | 1230.60] loss=2.53 avg=2.81\n",
            "[254 | 1235.29] loss=2.44 avg=2.80\n",
            "[255 | 1239.97] loss=2.56 avg=2.80\n",
            "[256 | 1244.70] loss=2.62 avg=2.80\n",
            "[257 | 1249.40] loss=2.65 avg=2.80\n",
            "[258 | 1254.08] loss=2.35 avg=2.79\n",
            "[259 | 1258.77] loss=2.69 avg=2.79\n",
            "[260 | 1263.46] loss=2.48 avg=2.79\n",
            "[261 | 1268.17] loss=2.70 avg=2.78\n",
            "[262 | 1272.86] loss=2.67 avg=2.78\n",
            "[263 | 1277.53] loss=2.60 avg=2.78\n",
            "[264 | 1282.23] loss=2.53 avg=2.78\n",
            "[265 | 1286.90] loss=2.67 avg=2.78\n",
            "[266 | 1291.58] loss=2.54 avg=2.78\n",
            "[267 | 1296.27] loss=2.57 avg=2.77\n",
            "[268 | 1300.96] loss=2.39 avg=2.77\n",
            "[269 | 1305.65] loss=2.84 avg=2.77\n",
            "[270 | 1310.35] loss=2.66 avg=2.77\n",
            "[271 | 1315.04] loss=2.65 avg=2.77\n",
            "[272 | 1319.72] loss=2.67 avg=2.77\n",
            "[273 | 1324.42] loss=2.80 avg=2.77\n",
            "[274 | 1329.12] loss=2.76 avg=2.77\n",
            "[275 | 1333.79] loss=2.64 avg=2.77\n",
            "[276 | 1338.49] loss=2.37 avg=2.76\n",
            "[277 | 1343.18] loss=2.68 avg=2.76\n",
            "[278 | 1347.87] loss=2.72 avg=2.76\n",
            "[279 | 1352.56] loss=2.55 avg=2.76\n",
            "[280 | 1357.24] loss=2.48 avg=2.75\n",
            "[281 | 1361.91] loss=2.60 avg=2.75\n",
            "[282 | 1366.60] loss=2.64 avg=2.75\n",
            "[283 | 1371.29] loss=2.58 avg=2.75\n",
            "[284 | 1376.01] loss=2.56 avg=2.75\n",
            "[285 | 1380.70] loss=2.47 avg=2.74\n",
            "[286 | 1385.38] loss=2.51 avg=2.74\n",
            "[287 | 1390.07] loss=2.75 avg=2.74\n",
            "[288 | 1394.77] loss=2.31 avg=2.74\n",
            "[289 | 1399.44] loss=2.54 avg=2.74\n",
            "[290 | 1404.14] loss=2.50 avg=2.73\n",
            "[291 | 1408.82] loss=2.57 avg=2.73\n",
            "[292 | 1413.51] loss=2.66 avg=2.73\n",
            "[293 | 1418.21] loss=2.55 avg=2.73\n",
            "[294 | 1422.90] loss=2.61 avg=2.73\n",
            "[295 | 1427.60] loss=2.61 avg=2.73\n",
            "[296 | 1432.31] loss=2.72 avg=2.73\n",
            "[297 | 1436.99] loss=2.52 avg=2.72\n",
            "[298 | 1441.67] loss=2.59 avg=2.72\n",
            "[299 | 1446.37] loss=2.87 avg=2.72\n",
            "[300 | 1451.08] loss=2.40 avg=2.72\n",
            "======== SAMPLE 1 ========\n",
            ", you may well say, he who looks to your worship to do me the favour to go out and see himself at the gate?\n",
            "“That, indeed, would be just enough to convince me,” said the curate, “and I would do well, señor, here, to let my father-in-law see himself at the gate; if I don’t tell him where I am going to see myself I may well hold my tongue and say so, which, in time, I hope is, shall not come out.”\n",
            "\n",
            "“To my belief it is in the great city of Valencia,” said Sancho, “where I have been.”\n",
            "\n",
            "“I think you mean to say there is a great city there,” said Don Quixote; “and this man here who goes to see himself in the gate is very much mistaken; for he is in the middle of a great procession, and, seeing the lady Dulcinea I will say with a straight face, he seems to have a great desire to see himself in the gate; so here you have a man of such great beauty in the midst of that procession, that the greatest pleasure he will feel in seeing himself at the gate is, in the words of one of the most eminent men in the world, ‘See how it is you and your lady Dulcinea that give a good account of the kingdom of Spain and her many and great men, and in the words of Sancho of La Mancha, ‘The most illustrious knight that has ever lived is the most well-bred lady he has ever seen, that is to say, the most beautiful lady of his race, because he has never seen anything of the kind that they say or say.”\n",
            "\n",
            "“You say so,” said Sancho.\n",
            "\n",
            "Dapple asked what Sancho said, and what he meant, and all they could say was, that it must be the same with these men.\n",
            "\n",
            "“We were all astonished,” said the curate.\n",
            "\n",
            "“By God’s grace it must be, I am,” said Sancho, “and I don’t know who they were, and even if they had been those of Rocinante I would not have believed them.”\n",
            "\n",
            "“Don Quixote’s father was very much troubled at what was going on,” agreed the damsel; and the two went on, and afterwards both made answer that the damsel must not enter the building until all was over; and so they went on to ask if they had any new money for their dinner, which the landlord of the garden-house, who was there with them, and of whom Sancho was very well taken, told them that all money was in his account, without any change taken from him.\n",
            "One man had given money on the spot on a bill of exchange of five lads-colts and eight swindles, which the damsel at the price of five swindles gave him in advance. Sancho said he would do as the man was very eager about money, and had another woman come to his village to give him money as well. The landlord of the garden-house then came in and asked Sancho the same thing that was said to the curate:\n",
            "\n",
            "“On what are you here, Señor Don Quixote, to ask me this favour?”\n",
            "\n",
            "“I am here here to-day,” he replied; “to put the questions aside, who are you? Do not come any more.”\n",
            "\n",
            "“If you come here,” said the curate, he would have no mind at all concerning money; but he must, in the end, and for his own part, ask the favours, money, and other questions on the lady Dulcinea.\n",
            "A very few of the curate’s servants asked him the same thing, and as they were all present he said the word ‘me.”\n",
            "\n",
            "“What word, my friend,” said he, “does the term mean?”\n",
            "\n",
            "“I can only tell you that as they are all present I am neither the lady of the castle, nor she who is my father, nor do they even recognise her as a lady; but I have a certain impression of the person to have become such a lady. All the servants, the curate, the servants, go in to ask questions and give the answer, which Sancho gave them.\n",
            "“This is the curate,” said Sancho; “he is very shrewd for an emperor.”\n",
            "\n",
            "“He has a fair deal of money on, señora,” said the curate, “and I must\n",
            "\n",
            "[301 | 1472.33] loss=2.64 avg=2.72\n",
            "[302 | 1477.03] loss=2.46 avg=2.72\n",
            "[303 | 1481.73] loss=2.56 avg=2.72\n",
            "[304 | 1486.45] loss=2.37 avg=2.71\n",
            "[305 | 1491.12] loss=2.53 avg=2.71\n",
            "[306 | 1495.79] loss=2.79 avg=2.71\n",
            "[307 | 1500.49] loss=2.58 avg=2.71\n",
            "[308 | 1505.18] loss=2.61 avg=2.71\n",
            "[309 | 1509.88] loss=2.38 avg=2.71\n",
            "[310 | 1514.57] loss=2.81 avg=2.71\n",
            "[311 | 1519.25] loss=2.68 avg=2.71\n",
            "[312 | 1523.96] loss=2.65 avg=2.71\n",
            "[313 | 1528.64] loss=2.62 avg=2.70\n",
            "[314 | 1533.32] loss=2.63 avg=2.70\n",
            "[315 | 1538.03] loss=2.50 avg=2.70\n",
            "[316 | 1542.73] loss=2.47 avg=2.70\n",
            "[317 | 1547.43] loss=2.80 avg=2.70\n",
            "[318 | 1552.14] loss=2.67 avg=2.70\n",
            "[319 | 1556.85] loss=2.57 avg=2.70\n",
            "[320 | 1561.54] loss=2.56 avg=2.70\n",
            "[321 | 1566.23] loss=2.46 avg=2.69\n",
            "[322 | 1570.93] loss=2.62 avg=2.69\n",
            "[323 | 1575.62] loss=2.38 avg=2.69\n",
            "[324 | 1580.30] loss=2.62 avg=2.69\n",
            "[325 | 1584.99] loss=2.41 avg=2.69\n",
            "[326 | 1589.69] loss=2.24 avg=2.68\n",
            "[327 | 1594.38] loss=2.45 avg=2.68\n",
            "[328 | 1599.06] loss=2.48 avg=2.68\n",
            "[329 | 1603.75] loss=2.51 avg=2.68\n",
            "[330 | 1608.43] loss=2.41 avg=2.67\n",
            "[331 | 1613.11] loss=2.41 avg=2.67\n",
            "[332 | 1617.83] loss=2.53 avg=2.67\n",
            "[333 | 1622.54] loss=2.36 avg=2.67\n",
            "[334 | 1627.23] loss=2.48 avg=2.66\n",
            "[335 | 1631.92] loss=2.54 avg=2.66\n",
            "[336 | 1636.63] loss=2.63 avg=2.66\n",
            "[337 | 1641.32] loss=2.54 avg=2.66\n",
            "[338 | 1646.00] loss=2.47 avg=2.66\n",
            "[339 | 1650.65] loss=2.84 avg=2.66\n",
            "[340 | 1655.34] loss=2.52 avg=2.66\n",
            "[341 | 1660.00] loss=2.60 avg=2.66\n",
            "[342 | 1664.68] loss=2.59 avg=2.66\n",
            "[343 | 1669.34] loss=2.62 avg=2.66\n",
            "[344 | 1674.03] loss=2.54 avg=2.66\n",
            "[345 | 1678.73] loss=2.63 avg=2.66\n",
            "[346 | 1683.42] loss=2.52 avg=2.66\n",
            "[347 | 1688.11] loss=2.66 avg=2.66\n",
            "[348 | 1692.79] loss=2.64 avg=2.65\n",
            "[349 | 1697.48] loss=2.41 avg=2.65\n",
            "[350 | 1702.18] loss=2.62 avg=2.65\n",
            "[351 | 1706.87] loss=2.64 avg=2.65\n",
            "[352 | 1711.55] loss=2.49 avg=2.65\n",
            "[353 | 1716.23] loss=2.36 avg=2.65\n",
            "[354 | 1720.92] loss=2.54 avg=2.65\n",
            "[355 | 1725.61] loss=2.45 avg=2.64\n",
            "[356 | 1730.27] loss=2.34 avg=2.64\n",
            "[357 | 1734.95] loss=2.41 avg=2.64\n",
            "[358 | 1739.65] loss=2.18 avg=2.63\n",
            "[359 | 1744.32] loss=2.19 avg=2.63\n",
            "[360 | 1749.01] loss=2.53 avg=2.63\n",
            "[361 | 1753.73] loss=2.49 avg=2.63\n",
            "[362 | 1758.42] loss=2.51 avg=2.63\n",
            "[363 | 1763.12] loss=2.56 avg=2.62\n",
            "[364 | 1767.82] loss=2.46 avg=2.62\n",
            "[365 | 1772.52] loss=2.55 avg=2.62\n",
            "[366 | 1777.21] loss=2.58 avg=2.62\n",
            "[367 | 1781.89] loss=2.31 avg=2.62\n",
            "[368 | 1786.59] loss=2.28 avg=2.62\n",
            "[369 | 1791.27] loss=2.33 avg=2.61\n",
            "[370 | 1795.99] loss=2.48 avg=2.61\n",
            "[371 | 1800.65] loss=2.65 avg=2.61\n",
            "[372 | 1805.32] loss=2.60 avg=2.61\n",
            "[373 | 1809.98] loss=2.57 avg=2.61\n",
            "[374 | 1814.67] loss=2.27 avg=2.61\n",
            "[375 | 1819.36] loss=2.55 avg=2.61\n",
            "[376 | 1824.05] loss=2.39 avg=2.60\n",
            "[377 | 1828.72] loss=2.33 avg=2.60\n",
            "[378 | 1833.41] loss=2.51 avg=2.60\n",
            "[379 | 1838.11] loss=2.40 avg=2.60\n",
            "[380 | 1842.81] loss=2.35 avg=2.60\n",
            "[381 | 1847.51] loss=2.25 avg=2.59\n",
            "[382 | 1852.18] loss=2.40 avg=2.59\n",
            "[383 | 1856.86] loss=2.32 avg=2.59\n",
            "[384 | 1861.54] loss=2.45 avg=2.59\n",
            "[385 | 1866.24] loss=2.55 avg=2.59\n",
            "[386 | 1870.91] loss=2.42 avg=2.58\n",
            "[387 | 1875.58] loss=2.74 avg=2.59\n",
            "[388 | 1880.28] loss=2.30 avg=2.58\n",
            "[389 | 1884.95] loss=2.47 avg=2.58\n",
            "[390 | 1889.63] loss=2.44 avg=2.58\n",
            "[391 | 1894.32] loss=2.50 avg=2.58\n",
            "[392 | 1898.98] loss=2.28 avg=2.58\n",
            "[393 | 1903.67] loss=2.43 avg=2.58\n",
            "[394 | 1908.34] loss=2.46 avg=2.57\n",
            "[395 | 1913.04] loss=2.49 avg=2.57\n",
            "[396 | 1917.73] loss=2.29 avg=2.57\n",
            "[397 | 1922.43] loss=2.37 avg=2.57\n",
            "[398 | 1927.14] loss=2.48 avg=2.57\n",
            "[399 | 1931.83] loss=2.59 avg=2.57\n",
            "[400 | 1936.53] loss=2.28 avg=2.56\n",
            "======== SAMPLE 1 ========\n",
            " the Sancho it is\n",
            "to the letter which your worship does not wish me to be governed; for\n",
            "if the government is something that has been proposed to me by Don Quixote of La\n",
            "Manche, you have no right to refuse me it by force; for, as you may well\n",
            "tell me, I have the honour of being governed not and ought not to be, provided\n",
            "I do not usurp or usurp the authority of a ruler, and, as\n",
            "they say, not a little the greater part gets the government, and that is a\n",
            "great government, and a government as small as one may be governors, to which the\n",
            "small ones are called governors, and some the great, governors with their councils, and\n",
            "they call in them the chief magistrates and judges of the city or villages, and the\n",
            "minors magistrates or judges; whereby the small ones, for my part, would be,\n",
            "indeed it might be, a king of the whole or most parts, and in the city,\n",
            "the magistrate or judge, but in the court of a small county, where the\n",
            "judge shall be the judge of all; for, the same thing being so, in one\n",
            "place all judges do but an awful lot of swearing. And now, as you, as governor\n",
            "of one city or county, I have a good will to obey, or a mind to leave my\n",
            "maintained will-power to your worship, unless that is to govern as\n",
            "happily and fairly as you please, for it seems your will is that of a\n",
            "rich-bearded governor-madman, whom the law says your will is bound to\n",
            "have for your lord; and, as governor of one city, I have a will-power\n",
            "to obey it, and will be persuaded that it will do honour to me\n",
            "and your worship to me; for so long as I live I swear against anybody who\n",
            "wishes to take me for a niece and abscondent, and do homage to\n",
            "my lord if and when he offers me a suitable wife, and will do\n",
            "everything in his power to serve me and his pleasure.\n",
            "\n",
            "“I have no intention of interfering with your pleasure or the\n",
            "honour of another, for that shall remain for the time being to have\n",
            "nothing to do with governorship, so long as I live as you govern me as\n",
            "long as you do me a favour or a service.”\n",
            "\n",
            "“I am all the government you require,” replied Sancho, “for it\n",
            "comes not to the governorships to confer privileges or titles on a\n",
            "governor; the power of one to whom it seems may extend to some other;\n",
            "for that is the most honourable I can bestow upon another. And now what\n",
            "do you want me to do, and does your worship desire me to do it?\n",
            "If it pleases you, do it, for there will be time enough for all;\n",
            "let us have it now and let there be no question left of the\n",
            "government; for it is always the lady whom God has chosen for the\n",
            "princely one, and the lady what my lady promises.”\n",
            "\n",
            "“By all that’s well,” said Don Quixote, “your worship, though there’s\n",
            "no need of any more questions, because God knows there is nothing else\n",
            "which he can do more conveniently than to make the lady the great lady\n",
            "of the hills say to her, “Abrasion, lady of the huts, it is all the\n",
            "business of the queen to make us feel satisfied with who we are.”\n",
            "\n",
            "“I take honour in asking what her Majesty requires of me,” said Sancho, “and when\n",
            "she asks, for example, her will, she has told me by no fault of my choosing\n",
            "which country my daughter will make my first lord, and if she’s not\n",
            "that of France the other of Greece, for a good governor will do all in his\n",
            "power. I’m told, for me I only ask what my daughter asks; and I’ll wait\n",
            "before answering. And so, if your worship pleases me, there’s no need of\n",
            "anymore inquiries or questions;” and Don Quixote asked his master if his\n",
            "lord and master had given him any advice about how to govern his island,\n",
            "and if so many things might be said about him who could govern, on which\n",
            "the answer, with the exception of what the governor told him would please\n",
            "the Queen and his lady, did not seem to him as much to ask as to\n",
            "answer.\n",
            "\n",
            "“As to the island,” said Sancho, “I say I believe, and God will understand,\n",
            "that you may know it and you’ll regret it.�\n",
            "\n",
            "[401 | 1957.75] loss=2.26 avg=2.56\n",
            "[402 | 1962.44] loss=2.48 avg=2.56\n",
            "[403 | 1967.12] loss=2.19 avg=2.56\n",
            "[404 | 1971.79] loss=2.09 avg=2.55\n",
            "[405 | 1976.49] loss=2.41 avg=2.55\n",
            "[406 | 1981.19] loss=2.42 avg=2.55\n",
            "[407 | 1985.87] loss=2.22 avg=2.55\n",
            "[408 | 1990.57] loss=2.26 avg=2.54\n",
            "[409 | 1995.25] loss=2.52 avg=2.54\n",
            "[410 | 1999.93] loss=2.32 avg=2.54\n",
            "[411 | 2004.64] loss=2.39 avg=2.54\n",
            "[412 | 2009.32] loss=2.41 avg=2.54\n",
            "[413 | 2014.02] loss=2.16 avg=2.53\n",
            "[414 | 2018.71] loss=2.43 avg=2.53\n",
            "[415 | 2023.40] loss=2.28 avg=2.53\n",
            "[416 | 2028.06] loss=2.34 avg=2.53\n",
            "[417 | 2032.75] loss=2.36 avg=2.53\n",
            "[418 | 2037.40] loss=2.43 avg=2.53\n",
            "[419 | 2042.08] loss=2.40 avg=2.52\n",
            "[420 | 2046.76] loss=2.36 avg=2.52\n",
            "[421 | 2051.45] loss=2.24 avg=2.52\n",
            "[422 | 2056.13] loss=1.96 avg=2.51\n",
            "[423 | 2060.80] loss=2.52 avg=2.51\n",
            "[424 | 2065.49] loss=2.31 avg=2.51\n",
            "[425 | 2070.17] loss=2.10 avg=2.51\n",
            "[426 | 2074.86] loss=2.38 avg=2.51\n",
            "[427 | 2079.55] loss=2.56 avg=2.51\n",
            "[428 | 2084.24] loss=2.50 avg=2.51\n",
            "[429 | 2088.90] loss=2.27 avg=2.50\n",
            "[430 | 2093.61] loss=2.65 avg=2.51\n",
            "[431 | 2098.28] loss=2.49 avg=2.51\n",
            "[432 | 2102.95] loss=2.50 avg=2.51\n",
            "[433 | 2107.62] loss=2.46 avg=2.51\n",
            "[434 | 2112.30] loss=2.38 avg=2.50\n",
            "[435 | 2116.99] loss=2.42 avg=2.50\n",
            "[436 | 2121.68] loss=2.55 avg=2.50\n",
            "[437 | 2126.36] loss=2.27 avg=2.50\n",
            "[438 | 2131.06] loss=2.35 avg=2.50\n",
            "[439 | 2135.75] loss=2.43 avg=2.50\n",
            "[440 | 2140.45] loss=2.01 avg=2.49\n",
            "[441 | 2145.16] loss=2.55 avg=2.50\n",
            "[442 | 2149.86] loss=2.43 avg=2.49\n",
            "[443 | 2154.54] loss=2.17 avg=2.49\n",
            "[444 | 2159.24] loss=2.32 avg=2.49\n",
            "[445 | 2163.95] loss=2.33 avg=2.49\n",
            "[446 | 2168.63] loss=2.11 avg=2.48\n",
            "[447 | 2173.32] loss=2.25 avg=2.48\n",
            "[448 | 2178.01] loss=2.17 avg=2.48\n",
            "[449 | 2182.68] loss=2.45 avg=2.48\n",
            "[450 | 2187.36] loss=2.24 avg=2.48\n",
            "[451 | 2192.04] loss=2.03 avg=2.47\n",
            "[452 | 2196.72] loss=2.06 avg=2.47\n",
            "[453 | 2201.39] loss=2.20 avg=2.46\n",
            "[454 | 2206.07] loss=2.16 avg=2.46\n",
            "[455 | 2210.75] loss=2.00 avg=2.46\n",
            "[456 | 2215.42] loss=2.33 avg=2.46\n",
            "[457 | 2220.11] loss=2.31 avg=2.45\n",
            "[458 | 2224.80] loss=2.49 avg=2.45\n",
            "[459 | 2229.46] loss=2.18 avg=2.45\n",
            "[460 | 2234.13] loss=2.55 avg=2.45\n",
            "[461 | 2238.84] loss=2.11 avg=2.45\n",
            "[462 | 2243.54] loss=2.50 avg=2.45\n",
            "[463 | 2248.24] loss=2.25 avg=2.45\n",
            "[464 | 2252.93] loss=2.26 avg=2.45\n",
            "[465 | 2257.63] loss=2.28 avg=2.44\n",
            "[466 | 2262.32] loss=2.13 avg=2.44\n",
            "[467 | 2267.01] loss=2.23 avg=2.44\n",
            "[468 | 2271.69] loss=1.87 avg=2.43\n",
            "[469 | 2276.38] loss=2.06 avg=2.43\n",
            "[470 | 2281.08] loss=2.19 avg=2.43\n",
            "[471 | 2285.75] loss=2.20 avg=2.42\n",
            "[472 | 2290.46] loss=1.98 avg=2.42\n",
            "[473 | 2295.14] loss=2.43 avg=2.42\n",
            "[474 | 2299.81] loss=1.93 avg=2.42\n",
            "[475 | 2304.51] loss=2.05 avg=2.41\n",
            "[476 | 2309.18] loss=2.18 avg=2.41\n",
            "[477 | 2313.85] loss=2.11 avg=2.41\n",
            "[478 | 2318.52] loss=2.54 avg=2.41\n",
            "[479 | 2323.21] loss=2.28 avg=2.41\n",
            "[480 | 2327.89] loss=2.06 avg=2.40\n",
            "[481 | 2332.58] loss=2.09 avg=2.40\n",
            "[482 | 2337.28] loss=2.44 avg=2.40\n",
            "[483 | 2341.97] loss=2.32 avg=2.40\n",
            "[484 | 2346.65] loss=1.89 avg=2.39\n",
            "[485 | 2351.33] loss=2.31 avg=2.39\n",
            "[486 | 2356.00] loss=2.60 avg=2.40\n",
            "[487 | 2360.69] loss=2.07 avg=2.39\n",
            "[488 | 2365.35] loss=2.31 avg=2.39\n",
            "[489 | 2370.04] loss=2.29 avg=2.39\n",
            "[490 | 2374.73] loss=2.39 avg=2.39\n",
            "[491 | 2379.41] loss=2.47 avg=2.39\n",
            "[492 | 2384.07] loss=2.24 avg=2.39\n",
            "[493 | 2388.76] loss=1.99 avg=2.39\n",
            "[494 | 2393.43] loss=2.26 avg=2.38\n",
            "[495 | 2398.13] loss=2.29 avg=2.38\n",
            "[496 | 2402.83] loss=2.31 avg=2.38\n",
            "[497 | 2407.51] loss=2.14 avg=2.38\n",
            "[498 | 2412.19] loss=2.17 avg=2.38\n",
            "[499 | 2416.88] loss=2.27 avg=2.38\n",
            "[500 | 2421.55] loss=2.25 avg=2.38\n",
            "Saving checkpoint/run1/model-500\n",
            "I can\n",
            "Tell thee, little girl, how I look upon thy\n",
            "permission to confess thyself the first to come, and\n",
            "set foot on the high road to it, to give thee entrance to\n",
            "the kingdom of Aragon or to France, where thou mayest always\n",
            "be the first to see me, and all the rest of it?”\n",
            "\n",
            "“And what are thy gifts of fortune?” said the princess.\n",
            "\n",
            "“I am a great beauty, sweet without shame, and I surpass in\n",
            "beauty all the princesses of the world, and in my youth all\n",
            "the princes of Spain had in their possession, and I am the fairest of\n",
            "them all, and of such a disposition and disposition that I am to\n",
            "fear God that I will not see thee in twelve years, or the life of\n",
            "the world, if thou wilt not give me thy promise of marriage.”\n",
            "\n",
            "“I swear, too,” replied the princess, “that I have nothing to do with marrying\n",
            "any one of thee, to save the other a second princess, a wife of mine,\n",
            "who is a very rich Christian, and a very valiant knight.”\n",
            "\n",
            "“That will do,” said I, “but I will take care that thou dost choose\n",
            "thy wife and her husband’s daughter, for the princesses of the world are\n",
            "always ready to give themselves a husband for their children.”\n",
            "\n",
            "“I don’t know how that can be,” said the princess, “but I know it\n",
            "will not be better than being married to a knight-errant, for there\n",
            "will be no want of knights-errant to satisfy the want of the princess.\n",
            "‘If your worship thinks that I am fit to be a squire to my lady,\n",
            "take care that thou seest that thou wilt give me more than three\n",
            "squires; for as I am a knight, I am not to be looked down upon or over\n",
            "given by the princesses of the world.’”\n",
            "\n",
            "“Full well then, little fool,” said I, “as I am well-nigh married to my\n",
            "wife, to whom I am not bound to marry, I am not in a position to\n",
            "take it for granted, and the best thing for me to do is to come\n",
            "before your worship and say that I am the best squire there is, and that\n",
            "it is not to be supposed that I am married to some feebleminded\n",
            "intellect, but to any kind of sensible woman with an enamoured mind; and\n",
            "here I am going to give you an answer that will please you, not to the\n",
            "folly of the nonsense that has been the subject of your worship in this\n",
            "house, but to the truth and good-breeding of the squires, and to the\n",
            "good-breeding of the children; and if you find yourself unable to make\n",
            "your choice, be glad to know that this time I am not to be blamed for it, for\n",
            "it was I who chose it, and I know you more than I know you; and I will give\n",
            "you the best squire there is, and that will be the happiest of all\n",
            "squires I know, as I am the most well-born in the world.”\n",
            "\n",
            "“That is true,” said I, “but I do not know how I can make a good squire,\n",
            "for I have heard of no one of the four in the world who is fit to be\n",
            "the squire of a knight-errant.”\n",
            "\n",
            "“I do not know how I can make a good squire,” said my master, “but one who\n",
            "will give him his liberty, and will give him his government, and will do\n",
            "as he pleases.”\n",
            "\n",
            "“That is true,” said my master, “but I do not know how I can give a\n",
            "squire, for I have heard many gentlemen say that if they had\n",
            "the least trouble they were much more likely to become knights-errant.\n",
            "But I do not think it is fair to say that I am the squire of a squire,\n",
            "because it does not seem to me by what I have seen how easy it is for\n",
            "a knight-errant to become a governor.”\n",
            "\n",
            "“I do not know how I can give a squire,” said my master, “but to me\n",
            "the squire of a knight-errant is the best squire which the world\n",
            "could wish for; and I, if I were not the best squire in the world,\n",
            "I would give him two, for all the squires in the world are squires, and\n",
            "the best squ\n"
          ]
        }
      ],
      "source": [
        "# This line is necessary to be able to run a new tf session if one has already been run\n",
        "tf.reset_default_graph()\n",
        "# Start a session\n",
        "sess = gpt2.start_tf_sess()\n",
        "# Fine tune `model_name` on `data`\n",
        "###################################\n",
        "# Swap out the `dataset` parameter with the path to your text dataset\n",
        "###################################\n",
        "gpt2.finetune(sess,\n",
        "              dataset='../content/996-0.txt',\n",
        "              model_name=model_name,\n",
        "              restore_from='latest',\n",
        "              steps=500)   # steps is max number of training steps\n",
        "\n",
        "gpt2.generate(sess, run_name='run1')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "sBOvJdJfkXIL",
        "kFoEeTYHDq2s",
        "7z6g7a_Y84n0",
        "TBigIUFTukeJ",
        "zcz0JGXjxFGe",
        "2vJVbYcAJAf2",
        "gMOzGDND9FD1",
        "eOtl8z8G9wbr",
        "2Krh0eYy18R9",
        "aEDv_-H7BvM0",
        "YH5mQBaa-_0b",
        "XMKRI77_-8nc",
        "qBT5jgifC7Im",
        "_K7F19SPQo6U",
        "9YLXvK51RnuL",
        "AHmjSVf_FNHv",
        "gPXJkNubFyY6",
        "iD45m3IwF9hh",
        "usQE-rSPZq_X",
        "IoA0tZZCa_1k",
        "8pa5vFJ5EUjv"
      ],
      "name": "lab9.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}